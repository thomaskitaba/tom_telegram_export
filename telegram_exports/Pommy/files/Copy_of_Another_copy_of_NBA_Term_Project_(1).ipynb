{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 0: Enviroment Setup"
      ],
      "metadata": {
        "id": "htr97dhz3DvE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QPoOxJGCzLsu",
        "outputId": "bfe93a3a-14d1-4307-986d-a6951755742a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pgmpy\n",
            "  Downloading pgmpy-1.0.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Collecting nba_api\n",
            "  Downloading nba_api-1.10.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pgmpy) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from pgmpy) (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from pgmpy) (2.8.0+cu126)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from pgmpy) (0.14.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from pgmpy) (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from pgmpy) (1.5.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.12/dist-packages (from pgmpy) (3.4.0)\n",
            "Collecting pyro-ppl (from pgmpy)\n",
            "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl->pgmpy)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (3.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pgmpy) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels->pgmpy) (1.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->pgmpy) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->pgmpy) (3.0.3)\n",
            "Downloading pgmpy-1.0.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nba_api-1.10.2-py3-none-any.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m287.0/287.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyro-api, fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein, nba_api, pyro-ppl, pgmpy\n",
            "Successfully installed Levenshtein-0.27.1 fuzzywuzzy-0.18.0 nba_api-1.10.2 pgmpy-1.0.0 pyro-api-0.1.2 pyro-ppl-1.9.1 python-Levenshtein-0.27.1 rapidfuzz-3.14.1\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install pgmpy pandas numpy matplotlib seaborn networkx fuzzywuzzy python-Levenshtein kaggle nba_api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "from pgmpy.models import BayesianNetwork\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator\n",
        "from pgmpy.inference import VariableElimination\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All packages installed and imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzSZBTaP1bCk",
        "outputId": "1842c794-1572-4046-950a-fe1a4035e31e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All packages installed and imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1: Data Acquistion & Problem Formalization"
      ],
      "metadata": {
        "id": "uXSCkKWm2-3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1.1: Install NBA API and Get Data\n"
      ],
      "metadata": {
        "id": "GJA6LCy83H_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üöÄ GETTING REAL NBA LINEUP DATA FROM OFFICIAL NBA API...\")\n",
        "\n",
        "# Install nba_api\n",
        "!pip install nba_api\n",
        "\n",
        "from nba_api.stats.endpoints import teamdashlineups\n",
        "from nba_api.stats.static import teams\n",
        "import pandas as pd\n",
        "\n",
        "# Get all NBA teams\n",
        "nba_teams = teams.get_teams()\n",
        "\n",
        "# Create team dictionary\n",
        "team_dict = {}\n",
        "for team in nba_teams:\n",
        "    team_name = team['full_name']\n",
        "    team_id = team['id']\n",
        "    team_dict[team_name] = team_id\n",
        "\n",
        "print(f\"‚úÖ Found {len(team_dict)} NBA teams\")\n",
        "\n",
        "# Function to get lineups for a team\n",
        "def get_lineups(team_id_i):\n",
        "    try:\n",
        "        lineup = teamdashlineups.TeamDashLineups(\n",
        "            team_id=team_id_i,\n",
        "            season='2023-24',  # Using 2023-24 for more complete data\n",
        "            season_type_all_star='Regular Season',\n",
        "            group_quantity=5,  # 5-man lineups\n",
        "            per_mode_detailed='Totals'\n",
        "        )\n",
        "        df = lineup.get_data_frames()\n",
        "        all_lineups = df[1]  # This contains the lineup data\n",
        "        return all_lineups\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error getting lineups for team {team_id_i}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Get lineups for all teams\n",
        "print(\"\\nüì• DOWNLOADING LINEUP DATA FOR ALL TEAMS...\")\n",
        "dataframes = []\n",
        "\n",
        "for i, team_name in enumerate(team_dict.keys()):\n",
        "    team_id_i = team_dict[team_name]\n",
        "    print(f\"   {i+1}/{len(team_dict)}: Getting {team_name}...\")\n",
        "\n",
        "    team_lineup = get_lineups(team_id_i)\n",
        "    if team_lineup is not None and not team_lineup.empty:\n",
        "        team_lineup['team'] = team_name\n",
        "        team_lineup['team_id'] = team_id_i\n",
        "        dataframes.append(team_lineup)\n",
        "\n",
        "    # Add small delay to avoid overwhelming API\n",
        "    import time\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Combine all team lineups\n",
        "if dataframes:\n",
        "    league_lineup = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    # Process the lineup data\n",
        "    league_lineup['players_list'] = league_lineup['GROUP_NAME'].str.split(' - ')\n",
        "\n",
        "    print(f\"\\n‚úÖ SUCCESS: Downloaded {len(league_lineup)} lineup combinations!\")\n",
        "    print(f\"üìä Dataset shape: {league_lineup.shape}\")\n",
        "\n",
        "    # Save the data\n",
        "    league_lineup.to_csv('nba_lineups_2024_api.csv', index=False)\n",
        "    print(\"üíæ Saved as 'nba_lineups_2024_api.csv'\")\n",
        "\n",
        "    # Show sample\n",
        "    print(\"\\nüîç SAMPLE OF REAL NBA LINEUP DATA:\")\n",
        "    display(league_lineup[['GROUP_NAME', 'team', 'MIN', 'PLUS_MINUS', 'FG_PCT', 'FG3_PCT']].head(3))\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No lineup data could be downloaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1qqh8cL-2qqX",
        "outputId": "f4cf0654-797f-48ed-e58e-87baf8f3de66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ GETTING REAL NBA LINEUP DATA FROM OFFICIAL NBA API...\n",
            "Requirement already satisfied: nba_api in /usr/local/lib/python3.12/dist-packages (1.10.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from nba_api) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from nba_api) (2.2.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from nba_api) (2.32.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->nba_api) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->nba_api) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->nba_api) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->nba_api) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->nba_api) (1.17.0)\n",
            "‚úÖ Found 30 NBA teams\n",
            "\n",
            "üì• DOWNLOADING LINEUP DATA FOR ALL TEAMS...\n",
            "   1/30: Getting Atlanta Hawks...\n",
            "   2/30: Getting Boston Celtics...\n",
            "   3/30: Getting Cleveland Cavaliers...\n",
            "   4/30: Getting New Orleans Pelicans...\n",
            "   5/30: Getting Chicago Bulls...\n",
            "   6/30: Getting Dallas Mavericks...\n",
            "   7/30: Getting Denver Nuggets...\n",
            "   8/30: Getting Golden State Warriors...\n",
            "   9/30: Getting Houston Rockets...\n",
            "   10/30: Getting Los Angeles Clippers...\n",
            "   11/30: Getting Los Angeles Lakers...\n",
            "   12/30: Getting Miami Heat...\n",
            "   13/30: Getting Milwaukee Bucks...\n",
            "   14/30: Getting Minnesota Timberwolves...\n",
            "   15/30: Getting Brooklyn Nets...\n",
            "   16/30: Getting New York Knicks...\n",
            "   17/30: Getting Orlando Magic...\n",
            "   18/30: Getting Indiana Pacers...\n",
            "   19/30: Getting Philadelphia 76ers...\n",
            "   20/30: Getting Phoenix Suns...\n",
            "   21/30: Getting Portland Trail Blazers...\n",
            "   22/30: Getting Sacramento Kings...\n",
            "   23/30: Getting San Antonio Spurs...\n",
            "   24/30: Getting Oklahoma City Thunder...\n",
            "   25/30: Getting Toronto Raptors...\n",
            "   26/30: Getting Utah Jazz...\n",
            "   27/30: Getting Memphis Grizzlies...\n",
            "   28/30: Getting Washington Wizards...\n",
            "   29/30: Getting Detroit Pistons...\n",
            "   30/30: Getting Charlotte Hornets...\n",
            "\n",
            "‚úÖ SUCCESS: Downloaded 7500 lineup combinations!\n",
            "üìä Dataset shape: (7500, 59)\n",
            "üíæ Saved as 'nba_lineups_2024_api.csv'\n",
            "\n",
            "üîç SAMPLE OF REAL NBA LINEUP DATA:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          GROUP_NAME           team  \\\n",
              "0  C. Capela - D. Murray - T. Young - S. Bey - J....  Atlanta Hawks   \n",
              "1  C. Capela - D. Murray - T. Young - D. Hunter -...  Atlanta Hawks   \n",
              "2  C. Capela - D. Murray - T. Young - D. Hunter -...  Atlanta Hawks   \n",
              "\n",
              "          MIN  PLUS_MINUS  FG_PCT  FG3_PCT  \n",
              "0  288.680000       -88.0   0.446    0.312  \n",
              "1  176.911667         8.0   0.468    0.384  \n",
              "2  171.505000       -26.0   0.464    0.367  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f2eba7b-813b-42aa-93da-ed7d789f1676\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GROUP_NAME</th>\n",
              "      <th>team</th>\n",
              "      <th>MIN</th>\n",
              "      <th>PLUS_MINUS</th>\n",
              "      <th>FG_PCT</th>\n",
              "      <th>FG3_PCT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C. Capela - D. Murray - T. Young - S. Bey - J....</td>\n",
              "      <td>Atlanta Hawks</td>\n",
              "      <td>288.680000</td>\n",
              "      <td>-88.0</td>\n",
              "      <td>0.446</td>\n",
              "      <td>0.312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C. Capela - D. Murray - T. Young - D. Hunter -...</td>\n",
              "      <td>Atlanta Hawks</td>\n",
              "      <td>176.911667</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C. Capela - D. Murray - T. Young - D. Hunter -...</td>\n",
              "      <td>Atlanta Hawks</td>\n",
              "      <td>171.505000</td>\n",
              "      <td>-26.0</td>\n",
              "      <td>0.464</td>\n",
              "      <td>0.367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f2eba7b-813b-42aa-93da-ed7d789f1676')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7f2eba7b-813b-42aa-93da-ed7d789f1676 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7f2eba7b-813b-42aa-93da-ed7d789f1676');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4e249a5d-5086-431a-bf3e-653b57103636\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e249a5d-5086-431a-bf3e-653b57103636')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4e249a5d-5086-431a-bf3e-653b57103636 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"\\u274c No lineup data could be downloaded\\\")\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"GROUP_NAME\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"C. Capela - D. Murray - T. Young - S. Bey - J. Johnson\",\n          \"C. Capela - D. Murray - T. Young - D. Hunter - S. Bey\",\n          \"C. Capela - D. Murray - T. Young - D. Hunter - J. Johnson\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"team\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Atlanta Hawks\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MIN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66.14551248906683,\n        \"min\": 171.505,\n        \"max\": 288.68,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          288.68\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PLUS_MINUS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 48.67579823005816,\n        \"min\": -88.0,\n        \"max\": 8.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -88.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FG_PCT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01171893055416464,\n        \"min\": 0.446,\n        \"max\": 0.468,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.446\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FG3_PCT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03763420430052074,\n        \"min\": 0.312,\n        \"max\": 0.384,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.312\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1.2: Analyze the API Data Structure"
      ],
      "metadata": {
        "id": "rofuD_Ck5wM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üî¨ ANALYZING NBA API DATA STRUCTURE...\")\n",
        "\n",
        "try:\n",
        "    lineup_data = pd.read_csv('nba_lineups_2024_api.csv')\n",
        "\n",
        "    print(\"üìã COLUMNS AVAILABLE:\")\n",
        "    for col in lineup_data.columns:\n",
        "        print(f\"   - {col}\")\n",
        "\n",
        "    print(\"\\nüéØ VARIABLES FOR OUR BAYESIAN NETWORK:\")\n",
        "\n",
        "    # Check for critical variables\n",
        "    critical_vars = {\n",
        "        'Efficiency (Target)': ['PLUS_MINUS', 'PTS'],\n",
        "        'Shooting': ['FG_PCT', 'FG3_PCT', 'EFG_PCT'],\n",
        "        'Playmaking': ['AST', 'AST_PCT'],\n",
        "        'Rebounding': ['OREB', 'DREB', 'REB'],\n",
        "        'Turnovers': ['TOV', 'TOV_PCT']\n",
        "    }\n",
        "\n",
        "    available_cols = lineup_data.columns.tolist()\n",
        "\n",
        "    for category, possible_vars in critical_vars.items():\n",
        "        found = [var for var in possible_vars if var in available_cols]\n",
        "        if found:\n",
        "            print(f\"   ‚úÖ {category}: {found}\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå {category}: Not found\")\n",
        "\n",
        "    print(f\"\\nüìä Dataset info: {lineup_data.shape}\")\n",
        "    print(f\"üë• Unique lineups: {lineup_data['GROUP_NAME'].nunique()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error analyzing data: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTmSlmAM4Yci",
        "outputId": "83a1fae7-45f3-4007-97b4-31fa5d010274"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¨ ANALYZING NBA API DATA STRUCTURE...\n",
            "üìã COLUMNS AVAILABLE:\n",
            "   - GROUP_SET\n",
            "   - GROUP_ID\n",
            "   - GROUP_NAME\n",
            "   - GP\n",
            "   - W\n",
            "   - L\n",
            "   - W_PCT\n",
            "   - MIN\n",
            "   - FGM\n",
            "   - FGA\n",
            "   - FG_PCT\n",
            "   - FG3M\n",
            "   - FG3A\n",
            "   - FG3_PCT\n",
            "   - FTM\n",
            "   - FTA\n",
            "   - FT_PCT\n",
            "   - OREB\n",
            "   - DREB\n",
            "   - REB\n",
            "   - AST\n",
            "   - TOV\n",
            "   - STL\n",
            "   - BLK\n",
            "   - BLKA\n",
            "   - PF\n",
            "   - PFD\n",
            "   - PTS\n",
            "   - PLUS_MINUS\n",
            "   - GP_RANK\n",
            "   - W_RANK\n",
            "   - L_RANK\n",
            "   - W_PCT_RANK\n",
            "   - MIN_RANK\n",
            "   - FGM_RANK\n",
            "   - FGA_RANK\n",
            "   - FG_PCT_RANK\n",
            "   - FG3M_RANK\n",
            "   - FG3A_RANK\n",
            "   - FG3_PCT_RANK\n",
            "   - FTM_RANK\n",
            "   - FTA_RANK\n",
            "   - FT_PCT_RANK\n",
            "   - OREB_RANK\n",
            "   - DREB_RANK\n",
            "   - REB_RANK\n",
            "   - AST_RANK\n",
            "   - TOV_RANK\n",
            "   - STL_RANK\n",
            "   - BLK_RANK\n",
            "   - BLKA_RANK\n",
            "   - PF_RANK\n",
            "   - PFD_RANK\n",
            "   - PTS_RANK\n",
            "   - PLUS_MINUS_RANK\n",
            "   - SUM_TIME_PLAYED\n",
            "   - team\n",
            "   - team_id\n",
            "   - players_list\n",
            "\n",
            "üéØ VARIABLES FOR OUR BAYESIAN NETWORK:\n",
            "   ‚úÖ Efficiency (Target): ['PLUS_MINUS', 'PTS']\n",
            "   ‚úÖ Shooting: ['FG_PCT', 'FG3_PCT']\n",
            "   ‚úÖ Playmaking: ['AST']\n",
            "   ‚úÖ Rebounding: ['OREB', 'DREB', 'REB']\n",
            "   ‚úÖ Turnovers: ['TOV']\n",
            "\n",
            "üìä Dataset info: (7500, 59)\n",
            "üë• Unique lineups: 7485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1.3: Integration with Kaggle Data"
      ],
      "metadata": {
        "id": "XAJTCj5c6LHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === PHASE 1.3 FIXED: USE ONLY NBA API DATA ===\n",
        "print(\"=== PHASE 1.3: PROPER NBA API DATA INTEGRATION ===\")\n",
        "\n",
        "# Load the NBA API data we just downloaded\n",
        "print(\"üì• Loading NBA API lineup data...\")\n",
        "lineup_data = pd.read_csv('nba_lineups_2024_api.csv')\n",
        "\n",
        "print(f\"üìä Original NBA API data: {lineup_data.shape}\")\n",
        "\n",
        "# Select only the variables we need for our Bayesian network\n",
        "print(\"\\nüéØ SELECTING VARIABLES FOR BAYESIAN NETWORK:\")\n",
        "selected_vars = {\n",
        "    'Efficiency': 'PLUS_MINUS',  # Net rating as efficiency proxy\n",
        "    'Shooting_FG': 'FG_PCT',     # Field goal percentage\n",
        "    'Shooting_3PT': 'FG3_PCT',   # 3-point percentage\n",
        "    'Playmaking': 'AST',         # Assists\n",
        "    'Turnovers': 'TOV',          # Turnovers\n",
        "    'Offensive_Rebounding': 'OREB'  # Offensive rebounds\n",
        "}\n",
        "\n",
        "# Create our feature dataset\n",
        "print(\"üîß Creating feature dataset from NBA API data...\")\n",
        "feature_data = lineup_data[list(selected_vars.values())].copy()\n",
        "feature_data.columns = list(selected_vars.keys())\n",
        "\n",
        "print(f\"üìä Feature dataset shape: {feature_data.shape}\")\n",
        "\n",
        "# Remove any missing values\n",
        "feature_data = feature_data.dropna()\n",
        "print(f\"üìä After removing missing values: {feature_data.shape}\")\n",
        "\n",
        "# Check data quality\n",
        "print(\"\\nüîç DATA QUALITY CHECK:\")\n",
        "print(\"Basic statistics:\")\n",
        "print(feature_data.describe())\n",
        "\n",
        "# Check for reasonable ranges (basketball logic)\n",
        "print(\"\\nüèÄ BASKETBALL LOGIC VALIDATION:\")\n",
        "print(\"Ranges should make sense for NBA:\")\n",
        "for col in feature_data.columns:\n",
        "    min_val = feature_data[col].min()\n",
        "    max_val = feature_data[col].max()\n",
        "    print(f\"  {col}: {min_val:.2f} to {max_val:.2f}\")\n",
        "\n",
        "# Verify we have enough data for discretization\n",
        "print(f\"\\nüìà DATA SUFFICIENCY:\")\n",
        "print(f\"  Total samples: {len(feature_data)}\")\n",
        "print(f\"  Minimum required: ~1,000 (for 3^5=243 combinations)\")\n",
        "print(f\"  Status: {'‚úÖ SUFFICIENT' if len(feature_data) >= 1000 else '‚ùå INSUFFICIENT'}\")\n",
        "\n",
        "if len(feature_data) >= 1000:\n",
        "    # Save the integrated data for Phase 2\n",
        "    feature_data.to_csv('nba_api_integrated_data.csv', index=False)\n",
        "    print(\"üíæ Saved integrated data as 'nba_api_integrated_data.csv'\")\n",
        "\n",
        "    print(\"\\n‚úÖ PHASE 1.3 COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"üéØ Using ONLY NBA API data for consistency\")\n",
        "    print(\"üöÄ Ready for Phase 2: Data Preprocessing\")\n",
        "else:\n",
        "    print(\"\\n‚ùå INSUFFICIENT DATA - Need to collect more NBA API data\")\n",
        "    print(\"   Consider multiple seasons or different API endpoints\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCNxl6HW6C69",
        "outputId": "6a388912-d009-4b9f-a6a9-ceeb56fdd935"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PHASE 1.3: PROPER NBA API DATA INTEGRATION ===\n",
            "üì• Loading NBA API lineup data...\n",
            "üìä Original NBA API data: (7500, 59)\n",
            "\n",
            "üéØ SELECTING VARIABLES FOR BAYESIAN NETWORK:\n",
            "üîß Creating feature dataset from NBA API data...\n",
            "üìä Feature dataset shape: (7500, 6)\n",
            "üìä After removing missing values: (7500, 6)\n",
            "\n",
            "üîç DATA QUALITY CHECK:\n",
            "Basic statistics:\n",
            "        Efficiency  Shooting_FG  Shooting_3PT   Playmaking    Turnovers  \\\n",
            "count  7500.000000  7500.000000   7500.000000  7500.000000  7500.000000   \n",
            "mean      0.555867     0.473603      0.354854     7.921333     3.910133   \n",
            "std      11.382578     0.151966      0.248258    21.286298     9.458745   \n",
            "min     -88.000000     0.000000      0.000000     0.000000     0.000000   \n",
            "25%      -5.000000     0.387000      0.200000     2.000000     1.000000   \n",
            "50%       0.000000     0.476000      0.333000     4.000000     2.000000   \n",
            "75%       5.000000     0.563000      0.500000     7.000000     4.000000   \n",
            "max     282.000000     1.000000      1.000000   615.000000   228.000000   \n",
            "\n",
            "       Offensive_Rebounding  \n",
            "count           7500.000000  \n",
            "mean               3.066533  \n",
            "std                7.285684  \n",
            "min                0.000000  \n",
            "25%                1.000000  \n",
            "50%                2.000000  \n",
            "75%                3.000000  \n",
            "max              216.000000  \n",
            "\n",
            "üèÄ BASKETBALL LOGIC VALIDATION:\n",
            "Ranges should make sense for NBA:\n",
            "  Efficiency: -88.00 to 282.00\n",
            "  Shooting_FG: 0.00 to 1.00\n",
            "  Shooting_3PT: 0.00 to 1.00\n",
            "  Playmaking: 0.00 to 615.00\n",
            "  Turnovers: 0.00 to 228.00\n",
            "  Offensive_Rebounding: 0.00 to 216.00\n",
            "\n",
            "üìà DATA SUFFICIENCY:\n",
            "  Total samples: 7500\n",
            "  Minimum required: ~1,000 (for 3^5=243 combinations)\n",
            "  Status: ‚úÖ SUFFICIENT\n",
            "üíæ Saved integrated data as 'nba_api_integrated_data.csv'\n",
            "\n",
            "‚úÖ PHASE 1.3 COMPLETED SUCCESSFULLY!\n",
            "üéØ Using ONLY NBA API data for consistency\n",
            "üöÄ Ready for Phase 2: Data Preprocessing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Data Preprocessing & Discretization\n"
      ],
      "metadata": {
        "id": "4nllLMTF7tti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2.1: Data Cleaning & Filtering"
      ],
      "metadata": {
        "id": "Vk1PULRsC_J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === PHASE 2.1 UPDATED: CLEAN NBA API DATA ===\n",
        "print(\"=== PHASE 2.1: CLEANING NBA API DATA ===\")\n",
        "\n",
        "# Load the integrated NBA API data\n",
        "print(\"üì• Loading integrated NBA API data...\")\n",
        "nba_api_data = pd.read_csv('nba_api_integrated_data.csv')\n",
        "\n",
        "print(f\"üìä Dataset shape: {nba_api_data.shape}\")\n",
        "print(f\"üéØ Columns: {list(nba_api_data.columns)}\")\n",
        "\n",
        "# The data is already clean (no missing values), but let's verify\n",
        "print(\"\\nüîç DATA CLEANLINESS CHECK:\")\n",
        "print(f\"Missing values: {nba_api_data.isnull().sum().sum()}\")  # Should be 0\n",
        "print(f\"Duplicate rows: {nba_api_data.duplicated().sum()}\")    # Should be minimal\n",
        "\n",
        "# Check for extreme outliers that might skew discretization\n",
        "print(\"\\nüìä OUTLIER DETECTION:\")\n",
        "for col in nba_api_data.columns:\n",
        "    Q1 = nba_api_data[col].quantile(0.25)\n",
        "    Q3 = nba_api_data[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = nba_api_data[(nba_api_data[col] < lower_bound) | (nba_api_data[col] > upper_bound)]\n",
        "    print(f\"  {col}: {len(outliers)} outliers ({len(outliers)/len(nba_api_data):.1%})\")\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 2.1 COMPLETED!\")\n",
        "print(\"üöÄ Ready for Phase 2.2: Feature Selection & Engineering\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJpoQpP_DJZk",
        "outputId": "5f067511-61ba-47bc-a945-fc30afa1586d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PHASE 2.1: CLEANING NBA API DATA ===\n",
            "üì• Loading integrated NBA API data...\n",
            "üìä Dataset shape: (7500, 6)\n",
            "üéØ Columns: ['Efficiency', 'Shooting_FG', 'Shooting_3PT', 'Playmaking', 'Turnovers', 'Offensive_Rebounding']\n",
            "\n",
            "üîç DATA CLEANLINESS CHECK:\n",
            "Missing values: 0\n",
            "Duplicate rows: 162\n",
            "\n",
            "üìä OUTLIER DETECTION:\n",
            "  Efficiency: 324 outliers (4.3%)\n",
            "  Shooting_FG: 234 outliers (3.1%)\n",
            "  Shooting_3PT: 343 outliers (4.6%)\n",
            "  Playmaking: 777 outliers (10.4%)\n",
            "  Turnovers: 628 outliers (8.4%)\n",
            "  Offensive_Rebounding: 729 outliers (9.7%)\n",
            "\n",
            "‚úÖ PHASE 2.1 COMPLETED!\n",
            "üöÄ Ready for Phase 2.2: Feature Selection & Engineering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2.2: Data Preprocessing & Engineering"
      ],
      "metadata": {
        "id": "g0tg2l2IDcF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === PHASE 2.2 FIXED: PROPER RATE STATISTICS ===\n",
        "print(\"=== PHASE 2.2 FIXED: PROPER RATE STATISTICS ===\")\n",
        "\n",
        "# We need the original lineup data with MINUTES to convert to rates\n",
        "print(\"üì• Loading full NBA lineup data with minutes...\")\n",
        "lineup_data = pd.read_csv('nba_lineups_2024_api.csv')\n",
        "\n",
        "print(\"üîß Converting totals to per-minute rates...\")\n",
        "\n",
        "# Calculate rates per 48 minutes (standard NBA rate)\n",
        "def calculate_rates(data):\n",
        "    rates_data = data.copy()\n",
        "\n",
        "    # Efficiency stays as PLUS_MINUS (already a rate)\n",
        "    rates_data['Efficiency'] = data['PLUS_MINUS']\n",
        "\n",
        "    # Convert totals to per-48-minute rates\n",
        "    minutes = data['MIN']\n",
        "\n",
        "    # Shooting percentages stay the same (already rates)\n",
        "    rates_data['Shooting_FG'] = data['FG_PCT']\n",
        "    rates_data['Shooting_3PT'] = data['FG3_PCT']\n",
        "\n",
        "    # Playmaking: Assists per 48 minutes\n",
        "    rates_data['Playmaking'] = (data['AST'] / minutes) * 48\n",
        "\n",
        "    # Turnovers: Turnovers per 48 minutes (INVERTED - lower is better)\n",
        "    rates_data['Turnovers'] = (data['TOV'] / minutes) * 48\n",
        "\n",
        "    # Offensive Rebounding: Offensive rebounds per 48 minutes\n",
        "    rates_data['Offensive_Rebounding'] = (data['OREB'] / minutes) * 48\n",
        "\n",
        "    return rates_data\n",
        "\n",
        "# Create rate-based features\n",
        "rates_data = calculate_rates(lineup_data)\n",
        "\n",
        "# Select only our 6 key variables\n",
        "rates_data = rates_data[['Efficiency', 'Shooting_FG', 'Shooting_3PT',\n",
        "                        'Playmaking', 'Turnovers', 'Offensive_Rebounding']]\n",
        "\n",
        "# Remove any infinite/NaN values from division\n",
        "rates_data = rates_data.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "print(f\"üìä Rate-based dataset shape: {rates_data.shape}\")\n",
        "\n",
        "# Check new correlations\n",
        "print(\"\\nüìä FIXED CORRELATIONS WITH EFFICIENCY:\")\n",
        "corr_matrix = rates_data.corr()\n",
        "efficiency_correlations = corr_matrix['Efficiency'].sort_values(ascending=False)\n",
        "\n",
        "for feature, corr in efficiency_correlations.items():\n",
        "    if feature != 'Efficiency':\n",
        "        print(f\"   {feature}: {corr:.3f}\")\n",
        "\n",
        "# Verify basketball logic is now correct\n",
        "positive_expected = ['Shooting_FG', 'Shooting_3PT', 'Playmaking', 'Offensive_Rebounding']\n",
        "negative_expected = ['Turnovers']\n",
        "\n",
        "actual_positive = [f for f in efficiency_correlations.index\n",
        "                  if f != 'Efficiency' and efficiency_correlations[f] > 0]\n",
        "actual_negative = [f for f in efficiency_correlations.index\n",
        "                  if f != 'Efficiency' and efficiency_correlations[f] < 0]\n",
        "\n",
        "print(f\"\\n‚úÖ Expected Positive: {positive_expected}\")\n",
        "print(f\"‚úÖ Expected Negative: {negative_expected}\")\n",
        "print(f\"üìä Actual Positive: {actual_positive}\")\n",
        "print(f\"üìä Actual Negative: {actual_negative}\")\n",
        "\n",
        "# Basketball logic validation\n",
        "if 'Turnovers' in actual_negative:\n",
        "    print(\"üéØ BASKETBALL LOGIC: Turnovers now negatively correlate with efficiency ‚úì\")\n",
        "else:\n",
        "    print(\"‚ùå BASKETBALL LOGIC STILL BROKEN - Need further investigation\")\n",
        "\n",
        "# Save the corrected data\n",
        "rates_data.to_csv('nba_api_corrected_rates.csv', index=False)\n",
        "print(\"\\nüíæ Saved corrected rate-based data as 'nba_api_corrected_rates.csv'\")\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 2.2 FIXED COMPLETED!\")\n",
        "print(\"üöÄ Ready for Phase 2.3 with proper basketball logic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hOKZBCpDOqe",
        "outputId": "8127238a-8d9c-4b34-8b85-7125994cff25"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PHASE 2.2 FIXED: PROPER RATE STATISTICS ===\n",
            "üì• Loading full NBA lineup data with minutes...\n",
            "üîß Converting totals to per-minute rates...\n",
            "üìä Rate-based dataset shape: (7500, 6)\n",
            "\n",
            "üìä FIXED CORRELATIONS WITH EFFICIENCY:\n",
            "   Shooting_FG: 0.329\n",
            "   Playmaking: 0.274\n",
            "   Shooting_3PT: 0.223\n",
            "   Offensive_Rebounding: 0.022\n",
            "   Turnovers: -0.141\n",
            "\n",
            "‚úÖ Expected Positive: ['Shooting_FG', 'Shooting_3PT', 'Playmaking', 'Offensive_Rebounding']\n",
            "‚úÖ Expected Negative: ['Turnovers']\n",
            "üìä Actual Positive: ['Shooting_FG', 'Playmaking', 'Shooting_3PT', 'Offensive_Rebounding']\n",
            "üìä Actual Negative: ['Turnovers']\n",
            "üéØ BASKETBALL LOGIC: Turnovers now negatively correlate with efficiency ‚úì\n",
            "\n",
            "üíæ Saved corrected rate-based data as 'nba_api_corrected_rates.csv'\n",
            "\n",
            "‚úÖ PHASE 2.2 FIXED COMPLETED!\n",
            "üöÄ Ready for Phase 2.3 with proper basketball logic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2.3: Discretization"
      ],
      "metadata": {
        "id": "V68gXp26D0RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === PHASE 2.3: SMART DISCRETIZATION ===\n",
        "print(\"=== PHASE 2.3: SMART DISCRETIZATION ===\")\n",
        "\n",
        "# Load the corrected rate-based data\n",
        "print(\"üì• Loading corrected rate-based data...\")\n",
        "rates_data = pd.read_csv('nba_api_corrected_rates.csv')\n",
        "\n",
        "print(f\"üìä Dataset shape: {rates_data.shape}\")\n",
        "print(\"üéØ Variables to discretize: Efficiency, Shooting_FG, Shooting_3PT, Playmaking, Turnovers, Offensive_Rebounding\")\n",
        "\n",
        "# Define basketball-informed discretization thresholds\n",
        "print(\"\\nüèÄ SETTING BASKETBALL-INFORMED THRESHOLDS:\")\n",
        "\n",
        "discretization_rules = {\n",
        "    'Efficiency': {\n",
        "        'description': 'Plus/Minus per game',\n",
        "        'Low': ('Below -5', 'Negative impact'),\n",
        "        'Medium': ('-5 to +5', 'Neutral impact'),\n",
        "        'High': ('Above +5', 'Positive impact')\n",
        "    },\n",
        "    'Shooting_FG': {\n",
        "        'description': 'Field Goal Percentage',\n",
        "        'Low': ('Below 45%', 'Poor shooting'),\n",
        "        'Medium': ('45% to 50%', 'Average shooting'),\n",
        "        'High': ('Above 50%', 'Elite shooting')\n",
        "    },\n",
        "    'Shooting_3PT': {\n",
        "        'description': '3-Point Percentage',\n",
        "        'Low': ('Below 35%', 'Poor 3PT'),\n",
        "        'Medium': ('35% to 40%', 'Average 3PT'),\n",
        "        'High': ('Above 40%', 'Elite 3PT')\n",
        "    },\n",
        "    'Playmaking': {\n",
        "        'description': 'Assists per 48 minutes',\n",
        "        'Low': ('Below 15', 'Low playmaking'),\n",
        "        'Medium': ('15 to 25', 'Average playmaking'),\n",
        "        'High': ('Above 25', 'High playmaking')\n",
        "    },\n",
        "    'Turnovers': {\n",
        "        'description': 'Turnovers per 48 minutes',\n",
        "        'Low': ('Below 10', 'Good ball control'),  # Lower turnovers = better\n",
        "        'Medium': ('10 to 15', 'Average ball control'),\n",
        "        'High': ('Above 15', 'Poor ball control')  # Higher turnovers = worse\n",
        "    },\n",
        "    'Offensive_Rebounding': {\n",
        "        'description': 'Offensive Rebounds per 48 minutes',\n",
        "        'Low': ('Below 8', 'Poor offensive rebounding'),\n",
        "        'Medium': ('8 to 12', 'Average offensive rebounding'),\n",
        "        'High': ('Above 12', 'Elite offensive rebounding')\n",
        "    }\n",
        "}\n",
        "\n",
        "# Apply discretization\n",
        "print(\"\\nüîß APPLYING DISCRETIZATION...\")\n",
        "final_discretized_data = rates_data.copy()\n",
        "\n",
        "for column in final_discretized_data.columns:\n",
        "    if column == 'Efficiency':\n",
        "        bins = [-float('inf'), -5, 5, float('inf')]\n",
        "        labels = ['Low', 'Medium', 'High']\n",
        "    elif column == 'Shooting_FG':\n",
        "        bins = [-float('inf'), 0.45, 0.50, float('inf')]\n",
        "        labels = ['Low', 'Medium', 'High']\n",
        "    elif column == 'Shooting_3PT':\n",
        "        bins = [-float('inf'), 0.35, 0.40, float('inf')]\n",
        "        labels = ['Low', 'Medium', 'High']\n",
        "    elif column == 'Playmaking':\n",
        "        bins = [-float('inf'), 15, 25, float('inf')]\n",
        "        labels = ['Low', 'Medium', 'High']\n",
        "    elif column == 'Turnovers':\n",
        "        bins = [-float('inf'), 10, 15, float('inf')]\n",
        "        labels = ['Low', 'Medium', 'High']  # Lower turnovers = \"Low\" category (good)\n",
        "    elif column == 'Offensive_Rebounding':\n",
        "        bins = [-float('inf'), 8, 12, float('inf')]\n",
        "        labels = ['Low', 'Medium', 'High']\n",
        "\n",
        "    final_discretized_data[column] = pd.cut(final_discretized_data[column], bins=bins, labels=labels)\n",
        "\n",
        "print(\"‚úÖ DISCRETIZATION COMPLETED!\")\n",
        "\n",
        "# Check the distribution of discretized variables\n",
        "print(\"\\nüìä DISCRETIZED DISTRIBUTIONS:\")\n",
        "for column in final_discretized_data.columns:\n",
        "    dist = final_discretized_data[column].value_counts(normalize=True).sort_index()\n",
        "    print(f\"{column}:\")\n",
        "    for state in ['Low', 'Medium', 'High']:\n",
        "        count = final_discretized_data[column].value_counts().get(state, 0)\n",
        "        percentage = dist.get(state, 0) * 100\n",
        "        print(f\"  {state}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "# Verify we have enough samples in each category\n",
        "print(\"\\nüîç SAMPLE SUFFICIENCY CHECK:\")\n",
        "min_samples = 100  # Minimum samples per category for reliable learning\n",
        "for column in final_discretized_data.columns:\n",
        "    for state in ['Low', 'Medium', 'High']:\n",
        "        count = (final_discretized_data[column] == state).sum()\n",
        "        if count < min_samples:\n",
        "            print(f\"‚ö†Ô∏è  {column}-{state}: Only {count} samples\")\n",
        "        else:\n",
        "            print(f\"‚úÖ {column}-{state}: {count} samples\")\n",
        "\n",
        "# Save the final discretized data\n",
        "final_discretized_data.to_csv('final_discretized_nba_data.csv', index=False)\n",
        "print(\"\\nüíæ Saved final discretized data as 'final_discretized_nba_data.csv'\")\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 2.3 COMPLETED!\")\n",
        "print(\"üöÄ Ready for Phase 2.4: Save Processed Data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15d8FAfrENif",
        "outputId": "2f8e818d-e2db-44f9-9a77-7e420ba43242"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PHASE 2.3: SMART DISCRETIZATION ===\n",
            "üì• Loading corrected rate-based data...\n",
            "üìä Dataset shape: (7500, 6)\n",
            "üéØ Variables to discretize: Efficiency, Shooting_FG, Shooting_3PT, Playmaking, Turnovers, Offensive_Rebounding\n",
            "\n",
            "üèÄ SETTING BASKETBALL-INFORMED THRESHOLDS:\n",
            "\n",
            "üîß APPLYING DISCRETIZATION...\n",
            "‚úÖ DISCRETIZATION COMPLETED!\n",
            "\n",
            "üìä DISCRETIZED DISTRIBUTIONS:\n",
            "Efficiency:\n",
            "  Low: 1906 samples (25.4%)\n",
            "  Medium: 3912 samples (52.2%)\n",
            "  High: 1682 samples (22.4%)\n",
            "Shooting_FG:\n",
            "  Low: 3229 samples (43.1%)\n",
            "  Medium: 1545 samples (20.6%)\n",
            "  High: 2726 samples (36.3%)\n",
            "Shooting_3PT:\n",
            "  Low: 3889 samples (51.9%)\n",
            "  Medium: 750 samples (10.0%)\n",
            "  High: 2861 samples (38.1%)\n",
            "Playmaking:\n",
            "  Low: 1385 samples (18.5%)\n",
            "  Medium: 2185 samples (29.1%)\n",
            "  High: 3930 samples (52.4%)\n",
            "Turnovers:\n",
            "  Low: 2707 samples (36.1%)\n",
            "  Medium: 1935 samples (25.8%)\n",
            "  High: 2858 samples (38.1%)\n",
            "Offensive_Rebounding:\n",
            "  Low: 3118 samples (41.6%)\n",
            "  Medium: 1565 samples (20.9%)\n",
            "  High: 2817 samples (37.6%)\n",
            "\n",
            "üîç SAMPLE SUFFICIENCY CHECK:\n",
            "‚úÖ Efficiency-Low: 1906 samples\n",
            "‚úÖ Efficiency-Medium: 3912 samples\n",
            "‚úÖ Efficiency-High: 1682 samples\n",
            "‚úÖ Shooting_FG-Low: 3229 samples\n",
            "‚úÖ Shooting_FG-Medium: 1545 samples\n",
            "‚úÖ Shooting_FG-High: 2726 samples\n",
            "‚úÖ Shooting_3PT-Low: 3889 samples\n",
            "‚úÖ Shooting_3PT-Medium: 750 samples\n",
            "‚úÖ Shooting_3PT-High: 2861 samples\n",
            "‚úÖ Playmaking-Low: 1385 samples\n",
            "‚úÖ Playmaking-Medium: 2185 samples\n",
            "‚úÖ Playmaking-High: 3930 samples\n",
            "‚úÖ Turnovers-Low: 2707 samples\n",
            "‚úÖ Turnovers-Medium: 1935 samples\n",
            "‚úÖ Turnovers-High: 2858 samples\n",
            "‚úÖ Offensive_Rebounding-Low: 3118 samples\n",
            "‚úÖ Offensive_Rebounding-Medium: 1565 samples\n",
            "‚úÖ Offensive_Rebounding-High: 2817 samples\n",
            "\n",
            "üíæ Saved final discretized data as 'final_discretized_nba_data.csv'\n",
            "\n",
            "‚úÖ PHASE 2.3 COMPLETED!\n",
            "üöÄ Ready for Phase 2.4: Save Processed Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2.4: Save Processed Data & Phase Completion"
      ],
      "metadata": {
        "id": "vnVHUPO7HFP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === PHASE 2.4: SAVE PROCESSED DATA ===\n",
        "print(\"=== PHASE 2.4: SAVE PROCESSED DATA ===\")\n",
        "\n",
        "# Verify the final dataset\n",
        "print(\"üîç FINAL DATASET VERIFICATION:\")\n",
        "print(f\"üìä Shape: {final_discretized_data.shape}\")\n",
        "print(f\"üéØ Columns: {list(final_discretized_data.columns)}\")\n",
        "print(f\"üìà Total samples: {len(final_discretized_data)}\")\n",
        "\n",
        "# Check data types and ensure proper categorical encoding\n",
        "print(\"\\nüîß DATA TYPE OPTIMIZATION:\")\n",
        "for col in final_discretized_data.columns:\n",
        "    unique_vals = final_discretized_data[col].unique()\n",
        "    print(f\"  {col}: {list(unique_vals)} - {final_discretized_data[col].dtype}\")\n",
        "\n",
        "# Convert to categorical with logical order for Bayesian network\n",
        "print(\"\\nüéØ OPTIMIZING FOR BAYESIAN NETWORK:\")\n",
        "final_processed_data = final_discretized_data.copy()\n",
        "\n",
        "# Ensure consistent categorical ordering\n",
        "for col in final_processed_data.columns:\n",
        "    final_processed_data[col] = pd.Categorical(\n",
        "        final_processed_data[col],\n",
        "        categories=['Low', 'Medium', 'High'],\n",
        "        ordered=True\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ All variables encoded as ordered categoricals\")\n",
        "\n",
        "# Final save\n",
        "final_processed_data.to_csv('nba_lineup_efficiency_final_data.csv', index=False)\n",
        "print(\"üíæ Saved as 'nba_lineup_efficiency_final_data.csv'\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nüìã FINAL DATASET SUMMARY:\")\n",
        "print(f\"‚úÖ Samples: {len(final_processed_data):,}\")\n",
        "print(f\"‚úÖ Features: {len(final_processed_data.columns)}\")\n",
        "print(f\"‚úÖ Data types: All categorical (Low/Medium/High)\")\n",
        "print(f\"‚úÖ Basketball logic: Preserved through discretization\")\n",
        "print(f\"‚úÖ Ready for Bayesian network training\")\n",
        "\n",
        "print(\"\\nüéâ PHASE 2 COMPLETED SUCCESSFULLY!\")\n",
        "print(\"üöÄ READY FOR PHASE 3: BAYESIAN NETWORK STRUCTURE & LEARNING\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm-B1fFyGZ4V",
        "outputId": "41ddc46e-8d66-4a9d-ff2d-d6feba8cc44a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PHASE 2.4: SAVE PROCESSED DATA ===\n",
            "üîç FINAL DATASET VERIFICATION:\n",
            "üìä Shape: (7500, 6)\n",
            "üéØ Columns: ['Efficiency', 'Shooting_FG', 'Shooting_3PT', 'Playmaking', 'Turnovers', 'Offensive_Rebounding']\n",
            "üìà Total samples: 7500\n",
            "\n",
            "üîß DATA TYPE OPTIMIZATION:\n",
            "  Efficiency: ['Low', 'High', 'Medium'] - category\n",
            "  Shooting_FG: ['Low', 'Medium', 'High'] - category\n",
            "  Shooting_3PT: ['Low', 'Medium', 'High'] - category\n",
            "  Playmaking: ['High', 'Medium', 'Low'] - category\n",
            "  Turnovers: ['High', 'Medium', 'Low'] - category\n",
            "  Offensive_Rebounding: ['High', 'Medium', 'Low'] - category\n",
            "\n",
            "üéØ OPTIMIZING FOR BAYESIAN NETWORK:\n",
            "‚úÖ All variables encoded as ordered categoricals\n",
            "üíæ Saved as 'nba_lineup_efficiency_final_data.csv'\n",
            "\n",
            "üìã FINAL DATASET SUMMARY:\n",
            "‚úÖ Samples: 7,500\n",
            "‚úÖ Features: 6\n",
            "‚úÖ Data types: All categorical (Low/Medium/High)\n",
            "‚úÖ Basketball logic: Preserved through discretization\n",
            "‚úÖ Ready for Bayesian network training\n",
            "\n",
            "üéâ PHASE 2 COMPLETED SUCCESSFULLY!\n",
            "üöÄ READY FOR PHASE 3: BAYESIAN NETWORK STRUCTURE & LEARNING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3: Bayesian Network Structure & Learning"
      ],
      "metadata": {
        "id": "7zPW9SS-HP41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3.1: Design the DAG Structure"
      ],
      "metadata": {
        "id": "qnPBeD5EHVOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === PHASE 3.1: HIERARCHICAL NETWORK STRUCTURE (RESTART) ===\n",
        "print(\"=== PHASE 3.1: HIERARCHICAL NETWORK STRUCTURE ===\")\n",
        "\n",
        "# Load the clean, processed data\n",
        "print(\"üì• Loading final processed data...\")\n",
        "final_data = pd.read_csv('nba_lineup_efficiency_final_data.csv')\n",
        "\n",
        "print(f\"üìä Dataset shape: {final_data.shape}\")\n",
        "print(f\"üéØ Columns: {list(final_data.columns)}\")\n",
        "\n",
        "# Verify data types are correct for Bayesian network\n",
        "print(\"\\nüîç DATA TYPE VERIFICATION:\")\n",
        "for col in final_data.columns:\n",
        "    print(f\"  {col}: {final_data[col].dtype} - {list(final_data[col].unique())}\")\n",
        "\n",
        "# Define the same hierarchical structure\n",
        "print(\"\\nüîó DESIGNING HIERARCHICAL STRUCTURE...\")\n",
        "print(\"üèÄ BASKETBALL LOGIC:\")\n",
        "print(\"  Level 0: Shooting_FG, Shooting_3PT, Playmaking, Turnovers, Offensive_Rebounding\")\n",
        "print(\"  Level 1: Shooting_Quality ‚Üê [FG + 3PT], Ball_Control ‚Üê [Playmaking - Turnovers]\")\n",
        "print(\"  Level 2: Efficiency ‚Üê [Shooting_Quality + Ball_Control + Second_Chances]\")\n",
        "\n",
        "from pgmpy.models import DiscreteBayesianNetwork\n",
        "\n",
        "# Create the hierarchical Bayesian Network structure\n",
        "hierarchical_model = DiscreteBayesianNetwork([\n",
        "    # Level 1: Intermediate basketball concepts\n",
        "    ('Shooting_FG', 'Shooting_Quality'),\n",
        "    ('Shooting_3PT', 'Shooting_Quality'),\n",
        "    ('Playmaking', 'Ball_Control'),\n",
        "    ('Turnovers', 'Ball_Control'),\n",
        "    ('Offensive_Rebounding', 'Second_Chances'),\n",
        "\n",
        "    # Level 2: Final efficiency\n",
        "    ('Shooting_Quality', 'Efficiency'),\n",
        "    ('Ball_Control', 'Efficiency'),\n",
        "    ('Second_Chances', 'Efficiency')\n",
        "])\n",
        "\n",
        "print(\"‚úÖ HIERARCHICAL NETWORK STRUCTURE CREATED!\")\n",
        "print(f\"üìà Nodes: {hierarchical_model.nodes()}\")\n",
        "print(f\"üìà Edges: {hierarchical_model.edges()}\")\n",
        "\n",
        "print(\"\\nüéØ MATHEMATICAL ADVANTAGE:\")\n",
        "print(\"  ‚Ä¢ 5 raw skills ‚Üí 3 intermediate concepts ‚Üí 1 target\")\n",
        "print(\"  ‚Ä¢ Reduces parameter complexity from 729 to 81\")\n",
        "print(\"  ‚Ä¢ 8.9x more data-efficient learning!\")\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 3.1 COMPLETED SUCCESSFULLY!\")\n",
        "print(\"üöÄ Ready for Phase 3.2: Learn CPTs with clean data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpKGtar8HLSq",
        "outputId": "c4a29e3a-53f0-4ea5-ebe1-307e1305ef23"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PHASE 3.1: HIERARCHICAL NETWORK STRUCTURE ===\n",
            "üì• Loading final processed data...\n",
            "üìä Dataset shape: (7500, 6)\n",
            "üéØ Columns: ['Efficiency', 'Shooting_FG', 'Shooting_3PT', 'Playmaking', 'Turnovers', 'Offensive_Rebounding']\n",
            "\n",
            "üîç DATA TYPE VERIFICATION:\n",
            "  Efficiency: object - ['Low', 'High', 'Medium']\n",
            "  Shooting_FG: object - ['Low', 'Medium', 'High']\n",
            "  Shooting_3PT: object - ['Low', 'Medium', 'High']\n",
            "  Playmaking: object - ['High', 'Medium', 'Low']\n",
            "  Turnovers: object - ['High', 'Medium', 'Low']\n",
            "  Offensive_Rebounding: object - ['High', 'Medium', 'Low']\n",
            "\n",
            "üîó DESIGNING HIERARCHICAL STRUCTURE...\n",
            "üèÄ BASKETBALL LOGIC:\n",
            "  Level 0: Shooting_FG, Shooting_3PT, Playmaking, Turnovers, Offensive_Rebounding\n",
            "  Level 1: Shooting_Quality ‚Üê [FG + 3PT], Ball_Control ‚Üê [Playmaking - Turnovers]\n",
            "  Level 2: Efficiency ‚Üê [Shooting_Quality + Ball_Control + Second_Chances]\n",
            "‚úÖ HIERARCHICAL NETWORK STRUCTURE CREATED!\n",
            "üìà Nodes: ['Shooting_FG', 'Shooting_Quality', 'Shooting_3PT', 'Playmaking', 'Ball_Control', 'Turnovers', 'Offensive_Rebounding', 'Second_Chances', 'Efficiency']\n",
            "üìà Edges: [('Shooting_FG', 'Shooting_Quality'), ('Shooting_Quality', 'Efficiency'), ('Shooting_3PT', 'Shooting_Quality'), ('Playmaking', 'Ball_Control'), ('Ball_Control', 'Efficiency'), ('Turnovers', 'Ball_Control'), ('Offensive_Rebounding', 'Second_Chances'), ('Second_Chances', 'Efficiency')]\n",
            "\n",
            "üéØ MATHEMATICAL ADVANTAGE:\n",
            "  ‚Ä¢ 5 raw skills ‚Üí 3 intermediate concepts ‚Üí 1 target\n",
            "  ‚Ä¢ Reduces parameter complexity from 729 to 81\n",
            "  ‚Ä¢ 8.9x more data-efficient learning!\n",
            "\n",
            "‚úÖ PHASE 3.1 COMPLETED SUCCESSFULLY!\n",
            "üöÄ Ready for Phase 3.2: Learn CPTs with clean data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3.2: Learn Conditional probability Tables (CPTs)"
      ],
      "metadata": {
        "id": "M7YvaRZfIvi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === PHASE 3.2 FIXED: BAYESIAN ESTIMATION WITH SMOOTHING ===\n",
        "print(\"=== PHASE 3.2 FIXED: BAYESIAN ESTIMATION WITH SMOOTHING ===\")\n",
        "\n",
        "print(\"üéØ Using Bayesian Estimation for better probability calibration...\")\n",
        "\n",
        "from pgmpy.estimators import BayesianEstimator\n",
        "\n",
        "# Create optimized intermediate variables\n",
        "print(\"üìä Creating optimized intermediate variables...\")\n",
        "hierarchical_data = final_data.copy()\n",
        "\n",
        "def create_optimized_intermediates(data):\n",
        "    \"\"\"Create intermediates with better basketball logic\"\"\"\n",
        "    results = data.copy()\n",
        "\n",
        "    # Use weighted scoring for more precision\n",
        "    score_map = {'Low': 0, 'Medium': 1, 'High': 2}\n",
        "\n",
        "    # Shooting: Weight FG% more than 3PT%\n",
        "    def shooting_quality(row):\n",
        "        fg_score = score_map[row['Shooting_FG']] * 1.5  # Weight FG% more\n",
        "        threept_score = score_map[row['Shooting_3PT']] * 1.0\n",
        "        total = fg_score + threept_score\n",
        "\n",
        "        if total >= 4.5:  # High threshold\n",
        "            return 'High'\n",
        "        elif total <= 1.5:  # Low threshold\n",
        "            return 'Low'\n",
        "        else:\n",
        "            return 'Medium'\n",
        "\n",
        "    # Ball Control: Strong emphasis on turnover avoidance\n",
        "    def ball_control(row):\n",
        "        pm_score = score_map[row['Playmaking']] * 1.0\n",
        "        to_score = (2 - score_map[row['Turnovers']]) * 1.5  # Weight turnovers heavier\n",
        "        total = pm_score + to_score\n",
        "\n",
        "        if total >= 3.5:\n",
        "            return 'High'\n",
        "        elif total <= 1.5:\n",
        "            return 'Low'\n",
        "        else:\n",
        "            return 'Medium'\n",
        "\n",
        "    # Second Chances: Direct but with efficiency guidance\n",
        "    def second_chances(row):\n",
        "        return row['Offensive_Rebounding']  # Keep it simple\n",
        "\n",
        "    results['Shooting_Quality'] = results.apply(shooting_quality, axis=1)\n",
        "    results['Ball_Control'] = results.apply(ball_control, axis=1)\n",
        "    results['Second_Chances'] = results.apply(second_chances, axis=1)\n",
        "\n",
        "    return results\n",
        "\n",
        "hierarchical_data = create_optimized_intermediates(hierarchical_data)\n",
        "\n",
        "print(\"‚úÖ Optimized intermediates created!\")\n",
        "print(f\"üìä Enhanced data shape: {hierarchical_data.shape}\")\n",
        "\n",
        "# Learn CPTs with BAYESIAN ESTIMATION (not MLE)\n",
        "print(\"\\nüéØ LEARNING CPTs WITH BAYESIAN ESTIMATION...\")\n",
        "print(\"   Using BDeu prior for smoother probability estimates...\")\n",
        "\n",
        "hierarchical_model.fit(\n",
        "    hierarchical_data,\n",
        "    estimator=BayesianEstimator,\n",
        "    prior_type='BDeu',\n",
        "    equivalent_sample_size=10  # Smoothing parameter\n",
        ")\n",
        "\n",
        "print(\"‚úÖ CPTs learned with Bayesian smoothing!\")\n",
        "\n",
        "# Create inference engine\n",
        "from pgmpy.inference import VariableElimination\n",
        "inference = VariableElimination(hierarchical_model)\n",
        "\n",
        "# Test accuracy with Bayesian estimation\n",
        "print(\"üìä TESTING BAYESIAN ESTIMATION ACCURACY...\")\n",
        "bayesian_predictions = []\n",
        "bayesian_true = []\n",
        "\n",
        "for idx, row in hierarchical_data.iterrows():\n",
        "    evidence = {\n",
        "        'Shooting_FG': row['Shooting_FG'],\n",
        "        'Shooting_3PT': row['Shooting_3PT'],\n",
        "        'Playmaking': row['Playmaking'],\n",
        "        'Turnovers': row['Turnovers'],\n",
        "        'Offensive_Rebounding': row['Offensive_Rebounding']\n",
        "    }\n",
        "    try:\n",
        "        result = inference.query(variables=['Efficiency'], evidence=evidence)\n",
        "        predicted = result.state_names['Efficiency'][result.values.argmax()]\n",
        "        bayesian_predictions.append(predicted)\n",
        "        bayesian_true.append(row['Efficiency'])\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "bayesian_accuracy = accuracy_score(bayesian_true, bayesian_predictions)\n",
        "\n",
        "# Check prediction distribution\n",
        "bayesian_pred_dist = pd.Series(bayesian_predictions).value_counts(normalize=True)\n",
        "\n",
        "print(f\"üéØ BAYESIAN ESTIMATION ACCURACY: {bayesian_accuracy:.1%}\")\n",
        "print(f\"üìä PREDICTION DISTRIBUTION: {dict(bayesian_pred_dist)}\")\n",
        "\n",
        "# Compare with previous approaches\n",
        "print(f\"\\nüìà ACCURACY COMPARISON:\")\n",
        "print(f\"  MLE Hierarchical: 53.9%\")\n",
        "print(f\"  MLE Direct: 58.2%\")\n",
        "print(f\"  BAYESIAN Hierarchical: {bayesian_accuracy:.1%}\")\n",
        "\n",
        "if bayesian_accuracy > 0.582:\n",
        "    improvement = (bayesian_accuracy - 0.582) * 100\n",
        "    print(f\"  ‚úÖ IMPROVEMENT: +{improvement:.1f}%\")\n",
        "\n",
        "# Basketball logic validation\n",
        "print(\"\\nüèÄ BAYESIAN MODEL BASKETBALL LOGIC:\")\n",
        "test_cases = [\n",
        "    (\"Elite Shooting\", {'Shooting_FG': 'High', 'Shooting_3PT': 'High'}),\n",
        "    (\"Great Ball Control\", {'Playmaking': 'High', 'Turnovers': 'Low'}),\n",
        "    (\"Championship Team\", {'Shooting_FG': 'High', 'Shooting_3PT': 'High', 'Playmaking': 'High', 'Turnovers': 'Low', 'Offensive_Rebounding': 'High'})\n",
        "]\n",
        "\n",
        "for name, evidence in test_cases:\n",
        "    result = inference.query(variables=['Efficiency'], evidence=evidence)\n",
        "    high_prob = result.values[result.state_names['Efficiency'].index('High')]\n",
        "    low_prob = result.values[result.state_names['Efficiency'].index('Low')]\n",
        "    print(f\"  {name}: P(High)={high_prob:.3f}, P(Low)={low_prob:.3f}\")\n",
        "\n",
        "if bayesian_accuracy > 0.65:\n",
        "    print(f\"\\nüéâ SUCCESS! Bayesian estimation achieves {bayesian_accuracy:.1%} accuracy!\")\n",
        "    print(\"üöÄ Ready for Phase 3.3 Validation\")\n",
        "else:\n",
        "    print(f\"\\nüîß Bayesian: {bayesian_accuracy:.1%} - Better but needs more work\")\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 3.2 FIXED COMPLETED!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s34xZx3EIAj7",
        "outputId": "c327c246-a50a-462b-b7e8-60a19267ef07"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PHASE 3.2 FIXED: BAYESIAN ESTIMATION WITH SMOOTHING ===\n",
            "üéØ Using Bayesian Estimation for better probability calibration...\n",
            "üìä Creating optimized intermediate variables...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pgmpy:Replacing existing CPD for Shooting_FG\n",
            "WARNING:pgmpy:Replacing existing CPD for Shooting_Quality\n",
            "WARNING:pgmpy:Replacing existing CPD for Efficiency\n",
            "WARNING:pgmpy:Replacing existing CPD for Shooting_3PT\n",
            "WARNING:pgmpy:Replacing existing CPD for Playmaking\n",
            "WARNING:pgmpy:Replacing existing CPD for Ball_Control\n",
            "WARNING:pgmpy:Replacing existing CPD for Turnovers\n",
            "WARNING:pgmpy:Replacing existing CPD for Offensive_Rebounding\n",
            "WARNING:pgmpy:Replacing existing CPD for Second_Chances\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Optimized intermediates created!\n",
            "üìä Enhanced data shape: (7500, 9)\n",
            "\n",
            "üéØ LEARNING CPTs WITH BAYESIAN ESTIMATION...\n",
            "   Using BDeu prior for smoother probability estimates...\n",
            "‚úÖ CPTs learned with Bayesian smoothing!\n",
            "üìä TESTING BAYESIAN ESTIMATION ACCURACY...\n",
            "üéØ BAYESIAN ESTIMATION ACCURACY: 55.1%\n",
            "üìä PREDICTION DISTRIBUTION: {'Medium': np.float64(0.7221333333333333), 'High': np.float64(0.1552), 'Low': np.float64(0.12266666666666666)}\n",
            "\n",
            "üìà ACCURACY COMPARISON:\n",
            "  MLE Hierarchical: 53.9%\n",
            "  MLE Direct: 58.2%\n",
            "  BAYESIAN Hierarchical: 55.1%\n",
            "\n",
            "üèÄ BAYESIAN MODEL BASKETBALL LOGIC:\n",
            "  Elite Shooting: P(High)=0.429, P(Low)=0.064\n",
            "  Great Ball Control: P(High)=0.288, P(Low)=0.170\n",
            "  Championship Team: P(High)=0.555, P(Low)=0.018\n",
            "\n",
            "üîß Bayesian: 55.1% - Better but needs more work\n",
            "\n",
            "‚úÖ PHASE 3.2 FIXED COMPLETED!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3.3: initial Model Validation"
      ],
      "metadata": {
        "id": "ibXNmMExJDI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === PHASE 3.3: INITIAL MODEL VALIDATION ===\n",
        "print(\"=== PHASE 3.3: INITIAL MODEL VALIDATION ===\")\n",
        "\n",
        "# TEST 1: MARGINAL PROBABILITIES\n",
        "print(\"\\nüìä MARGINAL PROBABILITIES:\")\n",
        "efficiency_marginal = inference.query(variables=['Efficiency'])\n",
        "print(\"Overall Efficiency Distribution:\")\n",
        "for state, prob in zip(efficiency_marginal.state_names['Efficiency'], efficiency_marginal.values):\n",
        "    print(f\"  P({state}): {prob:.3f}\")\n",
        "\n",
        "# TEST 2: REAL-WORLD BASKETBALL SCENARIOS\n",
        "print(\"\\nüèÄ REAL-WORLD SCENARIOS:\")\n",
        "\n",
        "# Championship team (elite everything)\n",
        "print(\"‚≠ê CHAMPIONSHIP TEAM (Elite across the board):\")\n",
        "evidence_champ = {\n",
        "    'Shooting_FG': 'High', 'Shooting_3PT': 'High',\n",
        "    'Playmaking': 'High', 'Turnovers': 'Low',\n",
        "    'Offensive_Rebounding': 'High'\n",
        "}\n",
        "result_champ = inference.query(variables=['Efficiency'], evidence=evidence_champ)\n",
        "champ_high = result_champ.values[result_champ.state_names['Efficiency'].index('High')]\n",
        "print(f\"  P(High Efficiency): {champ_high:.3f}\")\n",
        "\n",
        "# Rebuilding team (poor everything)\n",
        "print(\"\\nüî® REBUILDING TEAM (Poor across the board):\")\n",
        "evidence_rebuild = {\n",
        "    'Shooting_FG': 'Low', 'Shooting_3PT': 'Low',\n",
        "    'Playmaking': 'Low', 'Turnovers': 'High',\n",
        "    'Offensive_Rebounding': 'Low'\n",
        "}\n",
        "result_rebuild = inference.query(variables=['Efficiency'], evidence=evidence_rebuild)\n",
        "rebuild_low = result_rebuild.values[result_rebuild.state_names['Efficiency'].index('Low')]\n",
        "print(f\"  P(Low Efficiency): {rebuild_low:.3f}\")\n",
        "\n",
        "# TEST 3: ACCURACY CHECK\n",
        "print(\"\\nüéØ TRAINING ACCURACY CHECK:\")\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "for idx, row in hierarchical_data.iterrows():\n",
        "    evidence = {\n",
        "        'Shooting_FG': row['Shooting_FG'],\n",
        "        'Shooting_3PT': row['Shooting_3PT'],\n",
        "        'Playmaking': row['Playmaking'],\n",
        "        'Turnovers': row['Turnovers'],\n",
        "        'Offensive_Rebounding': row['Offensive_Rebounding']\n",
        "    }\n",
        "    try:\n",
        "        result = inference.query(variables=['Efficiency'], evidence=evidence)\n",
        "        predicted = result.state_names['Efficiency'][result.values.argmax()]\n",
        "        predictions.append(predicted)\n",
        "        true_labels.append(row['Efficiency'])\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"üéØ TRAINING ACCURACY: {accuracy:.1%}\")\n",
        "\n",
        "print(\"\\nüìä DETAILED PERFORMANCE:\")\n",
        "print(classification_report(true_labels, predictions, target_names=['High', 'Medium', 'Low']))\n",
        "\n",
        "# Compare with previous attempts\n",
        "print(f\"\\nüìà ACCURACY IMPROVEMENT:\")\n",
        "print(f\"  Previous Best: 54.9%\")\n",
        "print(f\"  Current: {accuracy:.1%}\")\n",
        "if accuracy > 0.549:\n",
        "    improvement = (accuracy - 0.549) * 100\n",
        "    print(f\"  ‚úÖ IMPROVEMENT: +{improvement:.1f}%\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  Still below previous best\")\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 3.3 COMPLETED!\")\n",
        "if accuracy > 0.60:\n",
        "    print(\"üöÄ EXCELLENT MODEL - Ready for Phase 4!\")\n",
        "else:\n",
        "    print(\"üîß Model needs tuning before Phase 4\")"
      ],
      "metadata": {
        "id": "ohARwRqwI08l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f58d267-c2b9-45cf-9d55-dcb9ea787f1f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PHASE 3.3: INITIAL MODEL VALIDATION ===\n",
            "\n",
            "üìä MARGINAL PROBABILITIES:\n",
            "Overall Efficiency Distribution:\n",
            "  P(High): 0.228\n",
            "  P(Low): 0.226\n",
            "  P(Medium): 0.546\n",
            "\n",
            "üèÄ REAL-WORLD SCENARIOS:\n",
            "‚≠ê CHAMPIONSHIP TEAM (Elite across the board):\n",
            "  P(High Efficiency): 0.561\n",
            "\n",
            "üî® REBUILDING TEAM (Poor across the board):\n",
            "  P(Low Efficiency): 0.725\n",
            "\n",
            "üéØ TRAINING ACCURACY CHECK:\n",
            "üéØ TRAINING ACCURACY: 53.9%\n",
            "\n",
            "üìä DETAILED PERFORMANCE:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        High       0.49      0.15      0.23      1682\n",
            "      Medium       0.55      0.30      0.39      1906\n",
            "         Low       0.54      0.82      0.65      3912\n",
            "\n",
            "    accuracy                           0.54      7500\n",
            "   macro avg       0.53      0.42      0.42      7500\n",
            "weighted avg       0.53      0.54      0.49      7500\n",
            "\n",
            "\n",
            "üìà ACCURACY IMPROVEMENT:\n",
            "  Previous Best: 54.9%\n",
            "  Current: 53.9%\n",
            "  ‚ö†Ô∏è  Still below previous best\n",
            "\n",
            "‚úÖ PHASE 3.3 COMPLETED!\n",
            "üîß Model needs tuning before Phase 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4: Model Inference & Validation"
      ],
      "metadata": {
        "id": "WL7TbwyrKCU6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D75zQkTuORwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b62f2a9f"
      },
      "source": [
        "# Task\n",
        "Adapt the existing notebook code to use the provided Kaggle datasets (\"games.csv\", \"games_details.csv\", and \"players.csv\") instead of the NBA API data. This involves loading the Kaggle data, merging it to create lineup-level statistics, cleaning and filtering this data, and then adapting the existing data preprocessing, Bayesian network learning, and model inference/validation steps to work with the Kaggle data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32b9123e"
      },
      "source": [
        "## Load kaggle data\n",
        "\n",
        "### Subtask:\n",
        "Load the relevant Kaggle datasets (`games.csv`, `games_details.csv`, and `players.csv`) into pandas DataFrames.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63749215"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to load the three specified Kaggle datasets. I will use pandas to read the CSV files into DataFrames and display their head and info to verify the loading was successful.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "772277e3"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the Kaggle data directory\n",
        "kaggle_data_path = 'kaggle_data'\n",
        "\n",
        "# Load the datasets\n",
        "games_df = pd.read_csv(os.path.join(kaggle_data_path, 'games.csv'))\n",
        "games_details_df = pd.read_csv(os.path.join(kaggle_data_path, 'games_details.csv'))\n",
        "players_df = pd.read_csv(os.path.join(kaggle_data_path, 'players.csv'))\n",
        "\n",
        "print(\"--- games.csv ---\")\n",
        "display(games_df.head())\n",
        "print(\"\\n--- games_details.csv ---\")\n",
        "display(games_details_df.head())\n",
        "print(\"\\n--- players.csv ---\")\n",
        "display(players_df.head())\n",
        "\n",
        "print(\"\\n--- Info for games.csv ---\")\n",
        "games_df.info()\n",
        "print(\"\\n--- Info for games_details.csv ---\")\n",
        "games_details_df.info()\n",
        "print(\"\\n--- Info for players.csv ---\")\n",
        "players_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d520510"
      },
      "source": [
        "## Merge kaggle data\n",
        "\n",
        "### Subtask:\n",
        "Join the `games` and `games_details` DataFrames to link game information with individual player performance in each game.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43f7e4d5"
      },
      "source": [
        "**Reasoning**:\n",
        "Merge the games and games_details dataframes using the GAME_ID column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c0e317c"
      },
      "source": [
        "# Merge games and games_details on GAME_ID\n",
        "merged_games_df = pd.merge(games_df, games_details_df, on='GAME_ID', how='inner')\n",
        "\n",
        "print(\"--- Merged games_df and games_details_df ---\")\n",
        "display(merged_games_df.head())\n",
        "print(f\"\\nShape of merged DataFrame: {merged_games_df.shape}\")\n",
        "print(\"\\n--- Info for merged_games_df ---\")\n",
        "merged_games_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5332feeb"
      },
      "source": [
        "## Create lineup data from kaggle\n",
        "\n",
        "### Subtask:\n",
        "Aggregate the player-level data from the merged DataFrame to create lineup-level statistics. This will involve identifying lineups within each game and summing their players' stats for the minutes they played together. This is a complex step as the Kaggle data doesn't explicitly provide lineup information; we'll need to simulate it or identify common on-court combinations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b83b56ff"
      },
      "source": [
        "**Reasoning**:\n",
        "Filter out rows with missing minutes, convert 'MIN' to numeric, and then group by game and team to prepare for lineup aggregation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "974bba3d"
      },
      "source": [
        "print(\"--- Aggregating player data to lineup level ---\")\n",
        "\n",
        "# 1. Filter to include only rows where player minutes ('MIN') are not null\n",
        "lineup_agg_df = merged_games_df[merged_games_df['MIN'].notnull()].copy()\n",
        "print(f\"Rows after filtering for non-null MIN: {len(lineup_agg_df)}\")\n",
        "\n",
        "# 2. Convert the 'MIN' column from object type to numeric\n",
        "# Handle potential errors by coercing non-numeric values to NaN and then dropping these rows.\n",
        "lineup_agg_df['MIN'] = pd.to_numeric(lineup_agg_df['MIN'], errors='coerce')\n",
        "lineup_agg_df.dropna(subset=['MIN'], inplace=True)\n",
        "print(f\"Rows after converting MIN to numeric and dropping NaNs: {len(lineup_agg_df)}\")\n",
        "\n",
        "# 3. Group the filtered DataFrame by 'GAME_ID' and 'TEAM_ID'\n",
        "# We will use this grouping to identify players who played in the same game for the same team.\n",
        "# The aggregation by MIN to simulate lineups will happen within this grouping.\n",
        "grouped_games_teams = lineup_agg_df.groupby(['GAME_ID', 'TEAM_ID'])\n",
        "\n",
        "print(\"\\nPrepared data for lineup aggregation by GAME_ID and TEAM_ID.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a28cca3"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the game and team groups, identify potential lineups by grouping players with the same minutes, aggregate stats for each lineup, calculate percentages, create a unique lineup identifier, and store the results in a new DataFrame as per steps 4-8 of the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbd8f048"
      },
      "source": [
        "print(\"--- Identifying lineups and aggregating stats ---\")\n",
        "\n",
        "lineup_stats_list = []\n",
        "\n",
        "# Iterate through each game and team\n",
        "for (game_id, team_id), group in grouped_games_teams:\n",
        "    # Within each group, identify potential lineups by grouping players with the same minutes\n",
        "    # Assume players with the same minutes in the same game for the same team played together in a lineup\n",
        "    lineups_in_game_team = group.groupby('MIN')\n",
        "\n",
        "    for min_played, lineup_group in lineups_in_game_team:\n",
        "        # Only consider groups with 5 players to represent 5-man lineups\n",
        "        if len(lineup_group) == 5:\n",
        "            # 5. Aggregate their statistics\n",
        "            aggregated_stats = {\n",
        "                'GAME_ID': game_id,\n",
        "                'TEAM_ID': team_id,\n",
        "                'MIN': min_played,\n",
        "                'PTS': lineup_group['PTS'].sum(),\n",
        "                'AST': lineup_group['AST'].sum(),\n",
        "                'REB': lineup_group['REB'].sum(),\n",
        "                'TOV': lineup_group['TOV'].sum(),\n",
        "                'STL': lineup_group['STL'].sum(),\n",
        "                'BLK': lineup_group['BLK'].sum(),\n",
        "                'PF': lineup_group['PF'].sum(),\n",
        "                'FGM': lineup_group['FGM'].sum(),\n",
        "                'FGA': lineup_group['FGA'].sum(),\n",
        "                'FG3M': lineup_group['FG3M'].sum(),\n",
        "                'FG3A': lineup_group['FG3A'].sum(),\n",
        "                'FTM': lineup_group['FTM'].sum(),\n",
        "                'FTA': lineup_group['FTA'].sum(),\n",
        "                'OREB': lineup_group['OREB'].sum(),\n",
        "                'DREB': lineup_group['DREB'].sum(),\n",
        "                'PLUS_MINUS': lineup_group['PLUS_MINUS'].sum() # Sum +/- for the lineup's duration\n",
        "            }\n",
        "\n",
        "            # 6. Calculate percentages\n",
        "            aggregated_stats['FG_PCT'] = aggregated_stats['FGA'] / aggregated_stats['FGM'] if aggregated_stats['FGM'] > 0 else 0\n",
        "            aggregated_stats['FG3_PCT'] = aggregated_stats['FG3A'] / aggregated_stats['FG3M'] if aggregated_stats['FG3M'] > 0 else 0\n",
        "            aggregated_stats['FT_PCT'] = aggregated_stats['FTA'] / aggregated_stats['FTM'] if aggregated_stats['FTM'] > 0 else 0\n",
        "\n",
        "            # 7. Create a unique identifier for each lineup\n",
        "            # Sort player IDs to ensure consistent lineup naming regardless of order\n",
        "            player_ids = sorted(lineup_group['PLAYER_ID'].tolist())\n",
        "            aggregated_stats['LINEUP_ID'] = '-'.join(map(str, player_ids))\n",
        "            aggregated_stats['PLAYER_IDS'] = player_ids # Keep list of player IDs\n",
        "\n",
        "            lineup_stats_list.append(aggregated_stats)\n",
        "\n",
        "# 8. Store the aggregated lineup statistics in a new DataFrame\n",
        "lineup_agg_df = pd.DataFrame(lineup_stats_list)\n",
        "\n",
        "# 9. Display the head and shape of the newly created lineup DataFrame\n",
        "print(\"\\n‚úÖ Lineup aggregation complete!\")\n",
        "print(\"\\n--- Aggregated Lineup Data ---\")\n",
        "display(lineup_agg_df.head())\n",
        "print(f\"\\nShape of aggregated lineup DataFrame: {lineup_agg_df.shape}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eefbf7ac"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the column 'TOV' was not found in the `lineup_group` DataFrame. This indicates that the column name in the `games_details.csv` file might be different or missing. I need to inspect the columns of `lineup_group` to identify the correct column name for turnovers and fix the aggregation step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruOA6A3ft99n"
      },
      "source": [
        "print(\"--- Debugging: Inspecting columns in lineup_group ---\")\n",
        "# Load the games_details_df again to check its columns\n",
        "games_details_df_check = pd.read_csv(os.path.join('kaggle_data', 'games_details.csv'))\n",
        "print(\"Columns in games_details.csv:\")\n",
        "print(games_details_df_check.columns.tolist())\n",
        "\n",
        "print(\"\\nColumns in the filtered data before grouping (lineup_agg_df):\")\n",
        "print(lineup_agg_df.columns.tolist())\n",
        "\n",
        "# Take a sample group to inspect its columns\n",
        "for (game_id, team_id), group in grouped_games_teams:\n",
        "    for min_played, lineup_group_sample in group.groupby('MIN'):\n",
        "        if len(lineup_group_sample) == 5:\n",
        "            print(f\"\\nColumns in a sample lineup_group (GAME_ID: {game_id}, TEAM_ID: {team_id}, MIN: {min_played}):\")\n",
        "            print(lineup_group_sample.columns.tolist())\n",
        "            break # Just need one sample\n",
        "    break # Just need one game/team group\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c679d53c"
      },
      "source": [
        "**Reasoning**:\n",
        "The debugging step revealed that the turnover column in `games_details.csv` and the merged DataFrame is named 'TO', not 'TOV'. The previous code needs to be corrected to use 'TO' instead of 'TOV' during the aggregation step. I will regenerate the code from step 4-8, correcting the column name and ensuring the percentage calculations handle the zero division correctly, then display the head and shape as per step 9.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50083fed"
      },
      "source": [
        "print(\"--- Identifying lineups and aggregating stats (Corrected) ---\")\n",
        "\n",
        "lineup_stats_list = []\n",
        "\n",
        "# Iterate through each game and team\n",
        "for (game_id, team_id), group in grouped_games_teams:\n",
        "    # Within each group, identify potential lineups by grouping players with the same minutes\n",
        "    # Assume players with the same minutes in the same game for the same team played together in a lineup\n",
        "    lineups_in_game_team = group.groupby('MIN')\n",
        "\n",
        "    for min_played, lineup_group in lineups_in_game_team:\n",
        "        # Only consider groups with 5 players to represent 5-man lineups\n",
        "        if len(lineup_group) == 5:\n",
        "            # 5. Aggregate their statistics - CORRECTED COLUMN NAME FOR TURNOVERS\n",
        "            aggregated_stats = {\n",
        "                'GAME_ID': game_id,\n",
        "                'TEAM_ID': team_id,\n",
        "                'MIN': min_played,\n",
        "                'PTS': lineup_group['PTS'].sum(),\n",
        "                'AST': lineup_group['AST'].sum(),\n",
        "                'REB': lineup_group['REB'].sum(),\n",
        "                'TOV': lineup_group['TO'].sum(), # Corrected: Use 'TO' instead of 'TOV'\n",
        "                'STL': lineup_group['STL'].sum(),\n",
        "                'BLK': lineup_group['BLK'].sum(),\n",
        "                'PF': lineup_group['PF'].sum(),\n",
        "                'FGM': lineup_group['FGM'].sum(),\n",
        "                'FGA': lineup_group['FGA'].sum(),\n",
        "                'FG3M': lineup_group['FG3M'].sum(),\n",
        "                'FG3A': lineup_group['FG3A'].sum(),\n",
        "                'FTM': lineup_group['FTM'].sum(),\n",
        "                'FTA': lineup_group['FTA'].sum(),\n",
        "                'OREB': lineup_group['OREB'].sum(),\n",
        "                'DREB': lineup_group['DREB'].sum(),\n",
        "                'PLUS_MINUS': lineup_group['PLUS_MINUS'].sum() # Sum +/- for the lineup's duration\n",
        "            }\n",
        "\n",
        "            # 6. Calculate percentages - Ensure handling of division by zero\n",
        "            aggregated_stats['FG_PCT'] = aggregated_stats['FGM'] / aggregated_stats['FGA'] if aggregated_stats['FGA'] > 0 else 0\n",
        "            aggregated_stats['FG3_PCT'] = aggregated_stats['FG3M'] / aggregated_stats['FG3A'] if aggregated_stats['FG3A'] > 0 else 0\n",
        "            aggregated_stats['FT_PCT'] = aggregated_stats['FTM'] / aggregated_stats['FTA'] if aggregated_stats['FTA'] > 0 else 0\n",
        "\n",
        "            # 7. Create a unique identifier for each lineup\n",
        "            # Sort player IDs to ensure consistent lineup naming regardless of order\n",
        "            player_ids = sorted(lineup_group['PLAYER_ID'].tolist())\n",
        "            aggregated_stats['LINEUP_ID'] = '-'.join(map(str, player_ids))\n",
        "            aggregated_stats['PLAYER_IDS'] = player_ids # Keep list of player IDs for potential later use\n",
        "\n",
        "            lineup_stats_list.append(aggregated_stats)\n",
        "\n",
        "# 8. Store the aggregated lineup statistics in a new DataFrame\n",
        "lineup_agg_df = pd.DataFrame(lineup_stats_list)\n",
        "\n",
        "# 9. Display the head and shape of the newly created lineup DataFrame\n",
        "print(\"\\n‚úÖ Lineup aggregation complete!\")\n",
        "print(\"\\n--- Aggregated Lineup Data ---\")\n",
        "display(lineup_agg_df.head())\n",
        "print(f\"\\nShape of aggregated lineup DataFrame: {lineup_agg_df.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d852a36b"
      },
      "source": [
        "## Clean and filter kaggle lineup data\n",
        "\n",
        "### Subtask:\n",
        "Apply cleaning and filtering steps similar to those in Phase 2.1, ensuring the Kaggle-based lineup data is ready for processing. This includes handling missing values and filtering by minutes played.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e487987"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply cleaning and filtering steps to the Kaggle-based lineup data, including handling missing values and filtering by minutes played, and report the changes in data shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3f29f92"
      },
      "source": [
        "print(\"=== Applying Cleaning and Filtering to Kaggle Lineup Data ===\")\n",
        "\n",
        "# Load the aggregated lineup data (created in the previous step)\n",
        "# We assume lineup_agg_df is already available from the previous steps.\n",
        "# If not, you would reload it here, e.g.:\n",
        "# lineup_agg_df = pd.read_csv('path_to_your_aggregated_lineup_data.csv')\n",
        "\n",
        "print(f\"\\nInitial data shape: {lineup_agg_df.shape}\")\n",
        "\n",
        "# Check for missing values before filtering\n",
        "print(\"\\nChecking missing values before filtering:\")\n",
        "missing_before_filter = lineup_agg_df.isnull().sum()\n",
        "print(missing_before_filter[missing_before_filter > 0])\n",
        "\n",
        "# Filter for meaningful lineups (played significant minutes)\n",
        "print(\"\\nApplying filter: MIN > 20\")\n",
        "filtered_kaggle_lineups = lineup_agg_df[lineup_agg_df['MIN'] > 20].copy()\n",
        "print(f\"Shape after filtering by MIN > 20: {filtered_kaggle_lineups.shape}\")\n",
        "\n",
        "# Check for missing values in critical columns after filtering\n",
        "critical_columns = ['PLUS_MINUS', 'FG_PCT', 'FG3_PCT', 'AST', 'OREB', 'TOV'] # Note: TOV was 'TO' in raw, but aggregated should be labeled TOV now\n",
        "print(f\"\\nChecking missing values in critical columns {critical_columns} after filtering:\")\n",
        "missing_after_filter = filtered_kaggle_lineups[critical_columns].isnull().sum()\n",
        "print(missing_after_filter[missing_after_filter > 0])\n",
        "\n",
        "\n",
        "# Remove any remaining rows with missing critical data\n",
        "print(f\"\\nRemoving rows with missing values in critical columns: {critical_columns}\")\n",
        "cleaned_kaggle_lineups = filtered_kaggle_lineups.dropna(subset=critical_columns).copy()\n",
        "\n",
        "print(f\"\\nFinal data shape after cleaning and filtering: {cleaned_kaggle_lineups.shape}\")\n",
        "\n",
        "# Display a sample of the cleaned data\n",
        "print(\"\\nSample of the cleaned and filtered Kaggle lineup data:\")\n",
        "display(cleaned_kaggle_lineups.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdf852e7"
      },
      "source": [
        "## Adapt phase 2 (data preprocessing & discretization) for kaggle data\n",
        "\n",
        "### Subtask:\n",
        "Modify the code in Phase 2 cells to use the Kaggle-based lineup data. Select equivalent columns (or the closest available) from the Kaggle data and apply the smart discretization logic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b055c45"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply the smart discretization functions to the relevant columns in the `cleaned_kaggle_lineups` DataFrame and create a new DataFrame with only the discretized columns, then display the head and value counts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0b281e3"
      },
      "source": [
        "print(\"=== Applying Smart Discretization to Kaggle Lineup Data ===\")\n",
        "\n",
        "# Ensure numpy is imported for np.select in discretization functions\n",
        "import numpy as np\n",
        "\n",
        "# Identify the columns in cleaned_kaggle_lineups corresponding to the original BN variables\n",
        "# PLUS_MINUS -> PLUS_MINUS (matches)\n",
        "# FG_PCT -> FG_PCT (matches)\n",
        "# FG3_PCT -> FG3_PCT (matches)\n",
        "# AST -> AST (matches)\n",
        "# OREB -> OREB (matches)\n",
        "# TOV -> TO (needs renaming to TOV for the function)\n",
        "\n",
        "# Rename the 'TO' column to 'TOV' to match the expected name in the discretization function\n",
        "if 'TO' in cleaned_kaggle_lineups.columns and 'TOV' not in cleaned_kaggle_lineups.columns:\n",
        "    cleaned_kaggle_lineups.rename(columns={'TO': 'TOV'}, inplace=True)\n",
        "    print(\"‚úÖ Renamed 'TO' column to 'TOV'.\")\n",
        "\n",
        "# Apply the smart discretization logic\n",
        "print(\"\\nüéØ APPLYING SMART DISCRETIZATION...\")\n",
        "\n",
        "kaggle_discretized_data = pd.DataFrame()\n",
        "\n",
        "# Discretize Efficiency\n",
        "kaggle_discretized_data['Efficiency'] = smart_discretize_efficiency(cleaned_kaggle_lineups)\n",
        "\n",
        "# Discretize Shooting\n",
        "kaggle_discretized_data['Shooting_FG'] = smart_discretize_shooting(cleaned_kaggle_lineups, 'FG_PCT')\n",
        "kaggle_discretized_data['Shooting_3PT'] = smart_discretize_shooting(cleaned_kaggle_lineups, 'FG3_PCT')\n",
        "\n",
        "# Discretize Count Stats (per minute)\n",
        "# Ensure MIN column is present for per-minute calculation\n",
        "if 'MIN' not in cleaned_kaggle_lineups.columns:\n",
        "     print(\"‚ùå Error: 'MIN' column not found in cleaned_kaggle_lineups for per-minute calculations.\")\n",
        "else:\n",
        "    kaggle_discretized_data['Playmaking'] = smart_discretize_count_stats_refined(cleaned_kaggle_lineups, 'AST')\n",
        "    kaggle_discretized_data['Offensive_Rebounding'] = smart_discretize_count_stats_refined(cleaned_kaggle_lineups, 'OREB')\n",
        "    kaggle_discretized_data['Turnovers'] = smart_discretize_count_stats_refined(cleaned_kaggle_lineups, 'TOV')\n",
        "\n",
        "print(\"\\n‚úÖ SMART DISCRETIZATION COMPLETE FOR KAGGLE DATA!\")\n",
        "\n",
        "# Display the head of the discretized data\n",
        "print(\"\\nüîç KAGGLE DISCRETIZED DATA PREVIEW:\")\n",
        "display(kaggle_discretized_data.head())\n",
        "\n",
        "# Print the value counts for each discretized column\n",
        "print(\"\\nüìä CATEGORY DISTRIBUTIONS IN KAGGLE DATA:\")\n",
        "for col in kaggle_discretized_data.columns:\n",
        "    distribution = kaggle_discretized_data[col].value_counts().sort_index()\n",
        "    print(f\"   {col}: {dict(distribution)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60df2810"
      },
      "source": [
        "## Adapt phase 3 (bayesian network structure & learning) for kaggle data\n",
        "\n",
        "### Subtask:\n",
        "Ensure the Bayesian Network structure and learning process use the discretized data derived from the Kaggle source.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a3857e1"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the kaggle_discretized_data, define the Bayesian Network structure, learn the CPTs from the data, and create the inference engine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9648442a"
      },
      "source": [
        "print(\"=== PHASE 3.2 (KAGGLE DATA): Learn Conditional Probability Tables (CPTs) ===\")\n",
        "\n",
        "# 1. Load the kaggle_discretized_data DataFrame\n",
        "# We assume kaggle_discretized_data is available from the previous step.\n",
        "# If not, you would load it here, e.g.:\n",
        "# kaggle_discretized_data = pd.read_csv('path_to_kaggle_discretized_data.csv')\n",
        "\n",
        "print(\"üîß Using kaggle_discretized_data for BN learning...\")\n",
        "print(f\"üìä Data shape: {kaggle_discretized_data.shape}\")\n",
        "print(f\"üéØ Columns: {list(kaggle_discretized_data.columns)}\")\n",
        "\n",
        "\n",
        "# 2. Redefine the hierarchical Bayesian Network structure\n",
        "print(\"\\nüîó Redefining hierarchical Bayesian network structure...\")\n",
        "from pgmpy.models import DiscreteBayesianNetwork\n",
        "\n",
        "# The hierarchical structure defined in Phase 3.1\n",
        "# Note: Intermediate nodes ('Shooting_Quality', 'Ball_Control', 'Second_Chances') are not directly\n",
        "# learned from Kaggle data, as the Kaggle data is already at the \"raw stats\" level.\n",
        "# However, the structure still assumes these intermediate concepts influence Efficiency.\n",
        "# We will learn the CPTs for the defined edges based on the direct relationships\n",
        "# available in the data. The interpretation of the intermediate nodes is conceptual\n",
        "# within the model structure itself.\n",
        "\n",
        "# The model should only include the nodes present in our kaggle_discretized_data\n",
        "# and the edges between them based on the hierarchical concept.\n",
        "# The direct dependencies from the raw stats to Efficiency in the original BN design\n",
        "# were removed in favor of the intermediate nodes.\n",
        "# We will use the edges that connect our available nodes.\n",
        "\n",
        "# Let's redefine edges based on direct relationships available in the discretized data\n",
        "# and the hierarchical idea:\n",
        "# Shooting_FG and Shooting_3PT influence Efficiency (conceptually via Shooting_Quality)\n",
        "# Playmaking and Turnovers influence Efficiency (conceptually via Ball_Control)\n",
        "# Offensive_Rebounding influences Efficiency (conceptually via Second_Chances)\n",
        "# So, the relevant edges connecting our observable nodes would be:\n",
        "# ('Shooting_FG', 'Efficiency'), ('Shooting_3PT', 'Efficiency')\n",
        "# ('Playmaking', 'Efficiency'), ('Turnovers', 'Efficiency')\n",
        "# ('Offensive_Rebounding', 'Efficiency')\n",
        "\n",
        "# This structure is the 'flat' version of the hierarchical model based on available data.\n",
        "# If we wanted the *exact* hierarchical model, we would need data for the intermediate nodes.\n",
        "# Since we don't have explicit intermediate nodes in the Kaggle data, we learn the CPTs\n",
        "# for Efficiency based on its direct parents in this \"flattened\" view, which corresponds\n",
        "# to the dependencies implied by the hierarchy where the intermediate nodes were removed.\n",
        "\n",
        "# Using the direct dependencies present in the data for learning:\n",
        "kaggle_bn_edges = [\n",
        "    ('Shooting_FG', 'Efficiency'),\n",
        "    ('Shooting_3PT', 'Efficiency'),\n",
        "    ('Playmaking', 'Efficiency'),\n",
        "    ('Turnovers', 'Efficiency'),\n",
        "    ('Offensive_Rebounding', 'Efficiency')\n",
        "]\n",
        "\n",
        "kaggle_hierarchical_model = DiscreteBayesianNetwork(kaggle_bn_edges)\n",
        "\n",
        "print(\"‚úÖ Hierarchical network structure defined for Kaggle data!\")\n",
        "print(f\"Nodes: {kaggle_hierarchical_model.nodes()}\")\n",
        "print(f\"Edges: {kaggle_hierarchical_model.edges()}\")\n",
        "\n",
        "\n",
        "# 3. Adapt the CPT learning step to fit the hierarchical_model using the kaggle_discretized_data DataFrame\n",
        "print(\"\\nüéØ Learning CPTs from kaggle_discretized_data...\")\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator # Using MLE for simplicity\n",
        "\n",
        "# Ensure the data only contains the nodes present in the model\n",
        "data_for_learning = kaggle_discretized_data[list(kaggle_hierarchical_model.nodes())].copy()\n",
        "\n",
        "kaggle_hierarchical_model.fit(data_for_learning, estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "print(\"‚úÖ CPTs successfully learned from Kaggle data!\")\n",
        "\n",
        "# 4. Create an inference engine\n",
        "print(\"\\nüîß Creating inference engine...\")\n",
        "from pgmpy.inference import VariableElimination\n",
        "kaggle_inference = VariableElimination(kaggle_hierarchical_model)\n",
        "\n",
        "print(\"‚úÖ Inference engine created!\")\n",
        "\n",
        "# Print confirmation messages\n",
        "print(\"\\nüéâ PHASE 3.2 (KAGGLE DATA) COMPLETED SUCCESSFULLY!\")\n",
        "print(\"Kaggle data loaded, BN structure defined, CPTs learned, and inference engine created.\")\n",
        "print(\"\\nüöÄ Ready for Phase 3.3 (Kaggle Data) Validation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03476a13"
      },
      "source": [
        "**Reasoning**:\n",
        "Perform initial model validation on the Bayesian Network learned from the Kaggle data by checking marginal probabilities and running basketball logic scenarios.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2ebc5d8"
      },
      "source": [
        "print(\"=== PHASE 3.3 (KAGGLE DATA): Initial Model Validation ===\")\n",
        "\n",
        "# TEST 1: MARGINAL PROBABILITIES\n",
        "print(\"\\nüìä MARGINAL PROBABILITIES (Kaggle Data):\")\n",
        "efficiency_marginal_kaggle = kaggle_inference.query(variables=['Efficiency'])\n",
        "print(\"Overall Efficiency Distribution:\")\n",
        "for state, prob in zip(efficiency_marginal_kaggle.state_names['Efficiency'], efficiency_marginal_kaggle.values):\n",
        "    print(f\"  P({state}): {prob:.3f}\")\n",
        "\n",
        "# TEST 2: BASKETBALL LOGIC SCENARIOS (Kaggle Data)\n",
        "print(\"\\nüèÄ BASKETBALL LOGIC TESTS (Kaggle Data):\")\n",
        "\n",
        "# Elite shooting team\n",
        "# Need to check if 'High' state exists for Shooting_FG and Shooting_3PT in Kaggle data\n",
        "if 'High' in kaggle_discretized_data['Shooting_FG'].unique() and 'High' in kaggle_discretized_data['Shooting_3PT'].unique():\n",
        "    evidence_elite_kaggle = {'Shooting_FG': 'High', 'Shooting_3PT': 'High'}\n",
        "    try:\n",
        "        result_elite_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence_elite_kaggle)\n",
        "        elite_high_kaggle = result_elite_kaggle.values[result_elite_kaggle.state_names['Efficiency'].index('High')]\n",
        "        print(f\"Elite Shooting ‚Üí P(High Efficiency): {elite_high_kaggle:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not query for Elite Shooting: {e}\")\n",
        "else:\n",
        "    print(\"  Cannot test Elite Shooting scenario: 'High' state not present in Kaggle data for Shooting_FG or Shooting_3PT.\")\n",
        "\n",
        "\n",
        "# Poor ball control team\n",
        "# Need to check if 'Low' state exists for Playmaking and 'High' for Turnovers\n",
        "if 'Low' in kaggle_discretized_data['Playmaking'].unique() and 'High' in kaggle_discretized_data['Turnovers'].unique():\n",
        "    evidence_poor_ball_kaggle = {'Playmaking': 'Low', 'Turnovers': 'High'}\n",
        "    try:\n",
        "        result_poor_ball_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence_poor_ball_kaggle)\n",
        "        poor_ball_low_kaggle = result_poor_ball_kaggle.values[result_poor_ball_kaggle.state_names['Efficiency'].index('Low')]\n",
        "        print(f\"Poor Ball Control ‚Üí P(Low Efficiency): {poor_ball_low_kaggle:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not query for Poor Ball Control: {e}\")\n",
        "else:\n",
        "     print(\"  Cannot test Poor Ball Control scenario: required states not present in Kaggle data for Playmaking or Turnovers.\")\n",
        "\n",
        "\n",
        "# TEST 3: NETWORK PATH VERIFICATION (Kaggle Data)\n",
        "print(\"\\nüîó NETWORK PATH VERIFICATION (Kaggle Data):\")\n",
        "paths_kaggle = [\n",
        "    (\"Shooting_FG ‚Üí Efficiency\", {'Shooting_FG': 'High'}),\n",
        "    (\"Shooting_3PT ‚Üí Efficiency\", {'Shooting_3PT': 'High'}),\n",
        "    (\"Playmaking ‚Üí Efficiency\", {'Playmaking': 'High'}),\n",
        "    (\"Turnovers ‚Üí Efficiency\", {'Turnovers': 'Low'}), # Low turnovers is good\n",
        "    (\"Offensive_Rebounding ‚Üí Efficiency\", {'Offensive_Rebounding': 'High'})\n",
        "]\n",
        "\n",
        "for path_name, evidence in paths_kaggle:\n",
        "    # Check if the evidence state exists in the data\n",
        "    valid_evidence = True\n",
        "    for var, state in evidence.items():\n",
        "        if state not in kaggle_discretized_data[var].unique():\n",
        "            print(f\"  Skipping '{path_name}': Evidence state '{state}' not present for '{var}' in Kaggle data.\")\n",
        "            valid_evidence = False\n",
        "            break\n",
        "\n",
        "    if valid_evidence:\n",
        "        try:\n",
        "            result_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence)\n",
        "            # Check if 'High' state exists in the query result\n",
        "            if 'High' in result_kaggle.state_names['Efficiency']:\n",
        "                high_prob_kaggle = result_kaggle.values[result_kaggle.state_names['Efficiency'].index('High')]\n",
        "                baseline_kaggle = efficiency_marginal_kaggle.values[efficiency_marginal_kaggle.state_names['Efficiency'].index('High')]\n",
        "                impact_kaggle = high_prob_kaggle - baseline_kaggle\n",
        "                print(f\"  {path_name}: +{impact_kaggle:.3f} {'‚úì' if impact_kaggle > 0 else '‚ùå'}\")\n",
        "            else:\n",
        "                 print(f\"  Skipping '{path_name}': 'High' state not in query result for Efficiency.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not query for '{path_name}': {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 3.3 (Kaggle Data) VALIDATION COMPLETED!\")\n",
        "print(\"Results provide initial insights into how variables influence Efficiency in the Kaggle data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7df6733"
      },
      "source": [
        "## Adapt phase 2 (data preprocessing & discretization) for kaggle data\n",
        "\n",
        "### Subtask:\n",
        "Adapt phase 2 (data preprocessing & discretization) for kaggle data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f737308e"
      },
      "source": [
        "**Reasoning**:\n",
        "Reload the cleaned Kaggle lineup data and examine the distribution of the raw numerical values to understand the limited variation and inform the revised discretization strategy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04850971"
      },
      "source": [
        "print(\"=== Revising Discretization for Kaggle Data ===\")\n",
        "\n",
        "# Reload the cleaned_kaggle_lineups DataFrame\n",
        "# Assuming it was saved or is available in the environment from previous steps\n",
        "# If not, you might need to recreate it from the merged data\n",
        "# For robustness, let's assume it's available as `cleaned_kaggle_lineups`\n",
        "\n",
        "print(\"üîß Examining raw data distribution before revised discretization...\")\n",
        "print(f\"Shape of cleaned_kaggle_lineups: {cleaned_kaggle_lineups.shape}\")\n",
        "\n",
        "# Re-examine the distribution of raw numerical values\n",
        "raw_columns_to_discretize = ['PLUS_MINUS', 'FG_PCT', 'FG3_PCT', 'AST', 'OREB', 'TOV'] # Using TOV after renaming\n",
        "\n",
        "print(\"\\nüìä Statistical summary of raw numerical data:\")\n",
        "display(cleaned_kaggle_lineups[raw_columns_to_discretize + ['MIN']].describe())\n",
        "\n",
        "print(\"\\nüìà Value counts for raw numerical data (showing unique values if few):\")\n",
        "for col in raw_columns_to_discretize:\n",
        "    unique_values = cleaned_kaggle_lineups[col].nunique()\n",
        "    print(f\"   {col}: {unique_values} unique values\")\n",
        "    if unique_values <= 15: # Show values if there are not too many unique ones\n",
        "        print(f\"      Unique values: {sorted(cleaned_kaggle_lineups[col].unique())}\")\n",
        "\n",
        "print(\"\\n‚úÖ Raw data examination complete. Planning revised discretization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "027b8234"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the raw data analysis, the 'PLUS_MINUS' column has only one unique value (0.0), making it impossible to create multiple categories for 'Efficiency'. Other columns have limited unique values. We need to revise the discretization strategy to handle this small dataset and lack of variation, potentially using simpler binning based on min/max for variables with more than one unique value, or acknowledging that 'Efficiency' cannot be discretized meaningfully with this specific Kaggle subset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaf74c1b"
      },
      "source": [
        "print(\"\\n=== Applying Revised Discretization for Kaggle Data ===\")\n",
        "print(\"Addressing the limited data points and single value in PLUS_MINUS.\")\n",
        "\n",
        "# Create fresh copy for revised discretization\n",
        "kaggle_discretized_data = pd.DataFrame()\n",
        "\n",
        "# Revised Discretization Strategy:\n",
        "# - For PLUS_MINUS: Given only one unique value (0.0), we cannot create multiple categories.\n",
        "#   We will assign a single category, but acknowledge this limitation.\n",
        "# - For other variables: Use simple binning based on min/max values if more than one unique value exists.\n",
        "#   Aim for 2 or 3 bins if feasible, prioritizing creating at least two categories.\n",
        "\n",
        "def revised_discretize_simple(data, column, num_bins=3, labels=['Low', 'Medium', 'High']):\n",
        "    \"\"\"Simple binning based on min/max for small datasets.\"\"\"\n",
        "    unique_values = data[column].nunique()\n",
        "    if unique_values <= 1:\n",
        "        print(f\"   {column}: Only 1 unique value ({data[column].iloc[0]}). Cannot discretize into multiple categories.\")\n",
        "        # Assign a single category, e.g., 'Single' or the label for the median bin\n",
        "        return pd.Series([labels[len(labels)//2]] * len(data), index=data.index)\n",
        "    elif unique_values < num_bins:\n",
        "         # If fewer unique values than bins, use unique values as categories or fewer bins\n",
        "         print(f\"   {column}: {unique_values} unique values. Using {unique_values} bins based on unique values.\")\n",
        "         bins = sorted(data[column].unique())\n",
        "         # Create labels based on the number of unique values\n",
        "         if unique_values == 2:\n",
        "             current_labels = ['Low', 'High']\n",
        "         else: # For 3 unique values, use default labels\n",
        "              current_labels = labels[:unique_values]\n",
        "\n",
        "         # Use cut with defined bins and labels\n",
        "         try:\n",
        "             discretized = pd.cut(data[column], bins=bins, labels=current_labels, include_lowest=True, right=False)\n",
        "             # For the last bin, need to handle the upper edge\n",
        "             if unique_values > 1:\n",
        "                  last_bin_label = current_labels[-1]\n",
        "                  # Find the maximum value and assign it to the last bin's label\n",
        "                  max_val_indices = data[data[column] == data[column].max()].index\n",
        "                  discretized.loc[max_val_indices] = last_bin_label\n",
        "         except Exception as e:\n",
        "              print(f\"Warning: Could not use unique values for binning {column}. Using simple cut.\")\n",
        "              discretized = pd.cut(data[column], bins=num_bins, labels=labels, include_lowest=True)\n",
        "\n",
        "    else:\n",
        "        # Use quantile-based or simple cut if enough unique values\n",
        "        try:\n",
        "            # Try quantile first for better distribution if possible\n",
        "            discretized = pd.qcut(data[column], q=num_bins, labels=labels, duplicates='drop')\n",
        "            if len(discretized.cat.categories) < num_bins:\n",
        "                 print(f\"   {column}: Quantile binning resulted in fewer than {num_bins} categories. Using simple cut.\")\n",
        "                 discretized = pd.cut(data[column], bins=num_bins, labels=labels, include_lowest=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Quantile binning failed for {column}. Using simple cut. Error: {e}\")\n",
        "            discretized = pd.cut(data[column], bins=num_bins, labels=labels, include_lowest=True)\n",
        "\n",
        "\n",
        "    print(f\"   {column}:\")\n",
        "    print(f\"      Range: {data[column].min():.2f} to {data[column].max():.2f}\")\n",
        "    print(f\"      Distribution: {discretized.value_counts().sort_index().to_dict()}\")\n",
        "    return discretized\n",
        "\n",
        "def revised_discretize_count_stats_simple(data, column, num_bins=3, labels=['Low', 'Medium', 'High']):\n",
        "    \"\"\"Simple binning for count stats per minute for small datasets.\"\"\"\n",
        "    # Convert to per-minute rates\n",
        "    # Handle cases where MIN might be 0, although filtered data should have MIN > 20\n",
        "    rate_data = data[column] / data['MIN'] if (data['MIN'] > 0).all() else data[column]\n",
        "\n",
        "    unique_values = rate_data.nunique()\n",
        "    if unique_values <= 1:\n",
        "        print(f\"   {column} (rate): Only 1 unique value ({rate_data.iloc[0]:.4f}). Cannot discretize into multiple categories.\")\n",
        "        # Assign a single category\n",
        "        return pd.Series([labels[len(labels)//2]] * len(data), index=data.index)\n",
        "    elif unique_values < num_bins:\n",
        "         # If fewer unique values than bins, use unique values as categories or fewer bins\n",
        "         print(f\"   {column} (rate): {unique_values} unique values. Using {unique_values} bins based on unique values.\")\n",
        "         bins = sorted(rate_data.unique())\n",
        "         if unique_values == 2:\n",
        "             current_labels = ['Low', 'High']\n",
        "         else:\n",
        "              current_labels = labels[:unique_values]\n",
        "\n",
        "         try:\n",
        "             discretized = pd.cut(rate_data, bins=bins, labels=current_labels, include_lowest=True, right=False)\n",
        "             if unique_values > 1:\n",
        "                  last_bin_label = current_labels[-1]\n",
        "                  max_val_indices = rate_data[rate_data == rate_data.max()].index\n",
        "                  discretized.loc[max_val_indices] = last_bin_label\n",
        "         except Exception as e:\n",
        "              print(f\"Warning: Could not use unique values for binning {column} rate. Using simple cut.\")\n",
        "              discretized = pd.cut(rate_data, bins=num_bins, labels=labels, include_lowest=True)\n",
        "\n",
        "    else:\n",
        "        # Use quantile-based or simple cut if enough unique values\n",
        "        try:\n",
        "            discretized = pd.qcut(rate_data, q=num_bins, labels=labels, duplicates='drop')\n",
        "            if len(discretized.cat.categories) < num_bins:\n",
        "                 print(f\"   {column} (rate): Quantile binning resulted in fewer than {num_bins} categories. Using simple cut.\")\n",
        "                 discretized = pd.cut(rate_data, bins=num_bins, labels=labels, include_lowest=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Quantile binning failed for {column} rate. Using simple cut. Error: {e}\")\n",
        "            discretized = pd.cut(rate_data, bins=num_bins, labels=labels, include_lowest=True)\n",
        "\n",
        "\n",
        "    # For TOV, 'Low' rate is good, 'High' rate is bad. Re-map if necessary.\n",
        "    if column == 'TOV' and isinstance(discretized.dtype, pd.CategoricalDtype):\n",
        "        # Check if labels are in expected order from qcut/cut (Low rate = 'Low' label)\n",
        "        if list(discretized.cat.categories) == ['Low', 'Medium', 'High']:\n",
        "             discretized = discretized.cat.rename_categories({'Low': 'High', 'Medium': 'Medium', 'High': 'Low'})\n",
        "             print(\"      Note: TOV labels reversed (Low rate = High category, High rate = Low category)\")\n",
        "        elif list(discretized.cat.categories) == ['Low', 'High']:\n",
        "             discretized = discretized.cat.rename_categories({'Low': 'High', 'High': 'Low'})\n",
        "             print(\"      Note: TOV labels reversed (Low rate = High category, High rate = Low category)\")\n",
        "\n",
        "\n",
        "    print(f\"   {column} (rate):\")\n",
        "    print(f\"      Range: {rate_data.min():.4f} to {rate_data.max():.4f}\")\n",
        "    if hasattr(discretized, 'cat'):\n",
        "         print(f\"      Categories: {list(discretized.cat.categories)}\")\n",
        "    print(f\"      Distribution: {discretized.value_counts().sort_index().to_dict()}\")\n",
        "\n",
        "\n",
        "    return discretized\n",
        "\n",
        "\n",
        "# Apply the revised simple discretization\n",
        "print(\"\\nüéØ APPLYING REVISED SIMPLE DISCRETIZATION...\")\n",
        "\n",
        "# Efficiency (PLUS_MINUS) - Will only have one category\n",
        "kaggle_discretized_data['Efficiency'] = revised_discretize_simple(cleaned_kaggle_lineups, 'PLUS_MINUS', num_bins=3)\n",
        "\n",
        "# Shooting Percentages\n",
        "kaggle_discretized_data['Shooting_FG'] = revised_discretize_simple(cleaned_kaggle_lineups, 'FG_PCT', num_bins=3)\n",
        "kaggle_discretized_data['Shooting_3PT'] = revised_discretize_simple(cleaned_kaggle_lineups, 'FG3_PCT', num_bins=3)\n",
        "\n",
        "# Count Stats (per minute)\n",
        "kaggle_discretized_data['Playmaking'] = revised_discretize_count_stats_simple(cleaned_kaggle_lineups, 'AST', num_bins=3)\n",
        "kaggle_discretized_data['Offensive_Rebounding'] = revised_discretize_count_stats_simple(cleaned_kaggle_lineups, 'OREB', num_bins=3)\n",
        "kaggle_discretized_data['Turnovers'] = revised_discretize_count_stats_simple(cleaned_kaggle_lineups, 'TOV', num_bins=3)\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ REVISED SIMPLE DISCRETIZATION COMPLETE FOR KAGGLE DATA!\")\n",
        "\n",
        "# Verify that each resulting categorical variable has at least two distinct states (except Efficiency)\n",
        "print(\"\\nüìä REVISED CATEGORY DISTRIBUTIONS IN KAGGLE DATA:\")\n",
        "for col in kaggle_discretized_data.columns:\n",
        "    distribution = kaggle_discretized_data[col].value_counts().sort_index()\n",
        "    print(f\"   {col}: {dict(distribution)}\")\n",
        "    if col != 'Efficiency' and len(distribution) < 2:\n",
        "        print(f\"      ‚ö†Ô∏è Warning: {col} has fewer than 2 categories after discretization.\")\n",
        "\n",
        "\n",
        "print(\"\\nüîç REVISED KAGGLE DATA PREVIEW:\")\n",
        "display(kaggle_discretized_data.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "504801db"
      },
      "source": [
        "## Adapt phase 3 (bayesian network structure & learning) for kaggle data\n",
        "\n",
        "### Subtask:\n",
        "Adapt phase 3 (bayesian network structure & learning) for kaggle data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66c6189e"
      },
      "source": [
        "**Reasoning**:\n",
        "Adapt Phase 3 (Bayesian Network Structure & Learning) to use the `kaggle_discretized_data` DataFrame, define the network structure, learn CPTs, and create an inference engine. This directly addresses the current subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4682e70"
      },
      "source": [
        "print(\"=== PHASE 3 (KAGGLE DATA): Bayesian Network Structure & Learning ===\")\n",
        "print(\"=== Sub-phase 3.1 & 3.2: Define Structure & Learn CPTs ===\")\n",
        "\n",
        "# 1. Load the kaggle_discretized_data DataFrame\n",
        "# We assume kaggle_discretized_data is available from the previous step.\n",
        "# If not, you would load it here, e.g.:\n",
        "# kaggle_discretized_data = pd.read_csv('path_to_kaggle_discretized_data.csv')\n",
        "\n",
        "print(\"üîß Using kaggle_discretized_data for BN learning...\")\n",
        "print(f\"üìä Data shape: {kaggle_discretized_data.shape}\")\n",
        "print(f\"üéØ Columns: {list(kaggle_discretized_data.columns)}\")\n",
        "\n",
        "\n",
        "# 2. Define the Bayesian Network structure\n",
        "print(\"\\nüîó Defining Bayesian network structure for Kaggle data...\")\n",
        "\n",
        "# Based on the available nodes in kaggle_discretized_data and the conceptual\n",
        "# hierarchical structure, we define the direct dependencies available in the data.\n",
        "# The intermediate nodes are implicit in this structure.\n",
        "# Edges: Statistical categories -> Efficiency\n",
        "kaggle_bn_edges = [\n",
        "    ('Shooting_FG', 'Efficiency'),\n",
        "    ('Shooting_3PT', 'Efficiency'),\n",
        "    ('Playmaking', 'Efficiency'),\n",
        "    ('Turnovers', 'Efficiency'),\n",
        "    ('Offensive_Rebounding', 'Efficiency')\n",
        "]\n",
        "\n",
        "kaggle_model = BayesianNetwork(kaggle_bn_edges)\n",
        "\n",
        "print(\"‚úÖ Bayesian network structure defined for Kaggle data!\")\n",
        "print(f\"Nodes: {kaggle_model.nodes()}\")\n",
        "print(f\"Edges: {kaggle_model.edges()}\")\n",
        "\n",
        "\n",
        "# 3. Adapt the CPT learning step\n",
        "print(\"\\nüéØ Learning CPTs from kaggle_discretized_data...\")\n",
        "\n",
        "# Ensure the data only contains the nodes present in the model\n",
        "data_for_learning = kaggle_discretized_data[list(kaggle_model.nodes())].copy()\n",
        "\n",
        "# Use MaximumLikelihoodEstimator to learn CPTs from the data\n",
        "kaggle_model.fit(data_for_learning, estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "print(\"‚úÖ CPTs successfully learned from Kaggle data!\")\n",
        "\n",
        "# 4. Create an inference engine\n",
        "print(\"\\nüîß Creating inference engine...\")\n",
        "kaggle_inference = VariableElimination(kaggle_model)\n",
        "\n",
        "print(\"‚úÖ Inference engine created!\")\n",
        "\n",
        "# Print confirmation messages for the subtask completion\n",
        "print(\"\\nüéâ SUBTASK: Adapt phase 3 (Bayesian Network Structure & Learning) for Kaggle data COMPLETED SUCCESSFULLY!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74793936"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `BayesianNetwork` was deprecated. The error message explicitly suggests using `DiscreteBayesianNetwork`. I need to correct the import and the class name used for creating the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjff13e_u6IU"
      },
      "source": [
        "print(\"=== PHASE 3 (KAGGLE DATA): Bayesian Network Structure & Learning ===\")\n",
        "print(\"=== Sub-phase 3.1 & 3.2: Define Structure & Learn CPTs ===\")\n",
        "\n",
        "# 1. Load the kaggle_discretized_data DataFrame\n",
        "# We assume kaggle_discretized_data is available in the environment from the previous step.\n",
        "# No need to load again if it's already a variable.\n",
        "\n",
        "print(\"üîß Using kaggle_discretized_data for BN learning...\")\n",
        "print(f\"üìä Data shape: {kaggle_discretized_data.shape}\")\n",
        "print(f\"üéØ Columns: {list(kaggle_discretized_data.columns)}\")\n",
        "\n",
        "\n",
        "# 2. Define the Bayesian Network structure (Corrected)\n",
        "print(\"\\nüîó Defining Bayesian network structure for Kaggle data...\")\n",
        "\n",
        "# Based on the available nodes in kaggle_discretized_data and the conceptual\n",
        "# hierarchical structure, we define the direct dependencies available in the data.\n",
        "# The intermediate nodes are implicit in this structure.\n",
        "# Edges: Statistical categories -> Efficiency\n",
        "kaggle_bn_edges = [\n",
        "    ('Shooting_FG', 'Efficiency'),\n",
        "    ('Shooting_3PT', 'Efficiency'),\n",
        "    ('Playmaking', 'Efficiency'),\n",
        "    ('Turnovers', 'Efficiency'),\n",
        "    ('Offensive_Rebounding', 'Efficiency')\n",
        "]\n",
        "\n",
        "# Use the correct class name: DiscreteBayesianNetwork\n",
        "from pgmpy.models import DiscreteBayesianNetwork\n",
        "\n",
        "kaggle_model = DiscreteBayesianNetwork(kaggle_bn_edges)\n",
        "\n",
        "print(\"‚úÖ Bayesian network structure defined for Kaggle data!\")\n",
        "print(f\"Nodes: {kaggle_model.nodes()}\")\n",
        "print(f\"Edges: {kaggle_model.edges()}\")\n",
        "\n",
        "\n",
        "# 3. Adapt the CPT learning step\n",
        "print(\"\\nüéØ Learning CPTs from kaggle_discretized_data...\")\n",
        "\n",
        "# Ensure the data only contains the nodes present in the model\n",
        "data_for_learning = kaggle_discretized_data[list(kaggle_model.nodes())].copy()\n",
        "\n",
        "# Use MaximumLikelihoodEstimator to learn CPTs from the data\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator # Keep import here for clarity within this block if it wasn't in the very first import cell\n",
        "\n",
        "kaggle_model.fit(data_for_learning, estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "print(\"‚úÖ CPTs successfully learned from Kaggle data!\")\n",
        "\n",
        "# 4. Create an inference engine\n",
        "print(\"\\nüîß Creating inference engine...\")\n",
        "from pgmpy.inference import VariableElimination # Keep import here for clarity\n",
        "\n",
        "kaggle_inference = VariableElimination(kaggle_model)\n",
        "\n",
        "print(\"‚úÖ Inference engine created!\")\n",
        "\n",
        "# Print confirmation messages for the subtask completion\n",
        "print(\"\\nüéâ SUBTASK: Adapt phase 3 (Bayesian Network Structure & Learning) for Kaggle data COMPLETED SUCCESSFULLY!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de85703a"
      },
      "source": [
        "## Adapt phase 4 (model inference & validation) for kaggle data\n",
        "\n",
        "### Subtask:\n",
        "Adapt the inference and validation code to work with the Kaggle-based model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47d56c33"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the validation steps for the Kaggle-based Bayesian Network model using the kaggle_inference engine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f4d3a4f"
      },
      "source": [
        "print(\"=== PHASE 4 (KAGGLE DATA): Model Inference & Validation ===\")\n",
        "print(\"=== Sub-phase 4.1: Sanity Checks ===\")\n",
        "\n",
        "# TEST 1: MARGINAL PROBABILITIES\n",
        "print(\"\\nüìä MARGINAL PROBABILITIES (Kaggle Data):\")\n",
        "# Check if the 'Efficiency' node exists in the model\n",
        "if 'Efficiency' in kaggle_model.nodes():\n",
        "    try:\n",
        "        efficiency_marginal_kaggle = kaggle_inference.query(variables=['Efficiency'])\n",
        "        print(\"Overall Efficiency Distribution:\")\n",
        "        for state, prob in zip(efficiency_marginal_kaggle.state_names['Efficiency'], efficiency_marginal_kaggle.values):\n",
        "            print(f\"  P({state}): {prob:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error querying marginal probability for Efficiency: {e}\")\n",
        "else:\n",
        "    print(\"  'Efficiency' node not found in the Kaggle model.\")\n",
        "\n",
        "\n",
        "# TEST 2: BASKETBALL LOGIC SCENARIOS (Kaggle Data)\n",
        "print(\"\\nüèÄ BASKETBALL LOGIC TESTS (Kaggle Data):\")\n",
        "\n",
        "# Elite shooting team\n",
        "evidence_elite_kaggle = {'Shooting_FG': 'High', 'Shooting_3PT': 'High'}\n",
        "valid_evidence_elite = True\n",
        "for var, state in evidence_elite_kaggle.items():\n",
        "    if var not in kaggle_model.nodes() or state not in kaggle_model.get_cardinality([var])[var]:\n",
        "        print(f\"  Cannot test Elite Shooting scenario: State '{state}' not present for '{var}' in Kaggle model.\")\n",
        "        valid_evidence_elite = False\n",
        "        break\n",
        "\n",
        "if valid_evidence_elite:\n",
        "    try:\n",
        "        result_elite_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence_elite_kaggle)\n",
        "        # Check if 'High' state exists in the query result for Efficiency\n",
        "        if 'High' in result_elite_kaggle.state_names['Efficiency']:\n",
        "            elite_high_kaggle = result_elite_kaggle.values[result_elite_kaggle.state_names['Efficiency'].index('High')]\n",
        "            print(f\"Elite Shooting ‚Üí P(High Efficiency): {elite_high_kaggle:.3f}\")\n",
        "        else:\n",
        "            print(f\"  'High' efficiency state not in result for Elite Shooting scenario.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not query for Elite Shooting: {e}\")\n",
        "\n",
        "# Poor ball control team\n",
        "evidence_poor_ball_kaggle = {'Playmaking': 'Low', 'Turnovers': 'High'}\n",
        "valid_evidence_poor_ball = True\n",
        "for var, state in evidence_poor_ball_kaggle.items():\n",
        "    if var not in kaggle_model.nodes() or state not in kaggle_model.get_cardinality([var])[var]:\n",
        "        print(f\"  Cannot test Poor Ball Control scenario: State '{state}' not present for '{var}' in Kaggle model.\")\n",
        "        valid_evidence_poor_ball = False\n",
        "        break\n",
        "\n",
        "if valid_evidence_poor_ball:\n",
        "    try:\n",
        "        result_poor_ball_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence_poor_ball_kaggle)\n",
        "        # Check if 'Low' state exists in the query result for Efficiency\n",
        "        if 'Low' in result_poor_ball_kaggle.state_names['Efficiency']:\n",
        "            poor_ball_low_kaggle = result_poor_ball_kaggle.values[result_poor_ball_kaggle.state_names['Efficiency'].index('Low')]\n",
        "            print(f\"Poor Ball Control ‚Üí P(Low Efficiency): {poor_ball_low_kaggle:.3f}\")\n",
        "        else:\n",
        "            print(f\"  'Low' efficiency state not in result for Poor Ball Control scenario.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not query for Poor Ball Control: {e}\")\n",
        "\n",
        "\n",
        "# TEST 3: NETWORK PATH VERIFICATION (Kaggle Data)\n",
        "print(\"\\nüîó NETWORK PATH VERIFICATION (Kaggle Data):\")\n",
        "paths_kaggle = [\n",
        "    (\"Shooting_FG ‚Üí Efficiency\", {'Shooting_FG': 'High'}),\n",
        "    (\"Shooting_3PT ‚Üí Efficiency\", {'Shooting_3PT': 'High'}),\n",
        "    (\"Playmaking ‚Üí Efficiency\", {'Playmaking': 'High'}),\n",
        "    (\"Turnovers ‚Üí Efficiency\", {'Turnovers': 'Low'}), # Low turnovers is good\n",
        "    (\"Offensive_Rebounding ‚Üí Efficiency\", {'Offensive_Rebounding': 'High'})\n",
        "]\n",
        "\n",
        "# Get baseline marginal probability for 'High' Efficiency if it exists\n",
        "baseline_kaggle_high = 0\n",
        "if 'Efficiency' in kaggle_model.nodes() and 'High' in kaggle_model.get_cardinality(['Efficiency'])['Efficiency']:\n",
        "     try:\n",
        "        baseline_marginal = kaggle_inference.query(variables=['Efficiency'])\n",
        "        baseline_kaggle_high = baseline_marginal.values[baseline_marginal.state_names['Efficiency'].index('High')]\n",
        "     except Exception as e:\n",
        "         print(f\"  Could not get baseline marginal for High Efficiency: {e}\")\n",
        "\n",
        "\n",
        "for path_name, evidence in paths_kaggle:\n",
        "    # Check if the evidence state exists in the model\n",
        "    valid_evidence = True\n",
        "    for var, state in evidence.items():\n",
        "        if var not in kaggle_model.nodes() or state not in kaggle_model.get_cardinality([var])[var]:\n",
        "            print(f\"  Skipping '{path_name}': Evidence state '{state}' not present for '{var}' in Kaggle model.\")\n",
        "            valid_evidence = False\n",
        "            break\n",
        "\n",
        "    if valid_evidence:\n",
        "        try:\n",
        "            result_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence)\n",
        "            # Check if 'High' state exists in the query result\n",
        "            if 'High' in result_kaggle.state_names['Efficiency']:\n",
        "                high_prob_kaggle = result_kaggle.values[result_kaggle.state_names['Efficiency'].index('High')]\n",
        "                impact_kaggle = high_prob_kaggle - baseline_kaggle_high\n",
        "                print(f\"  {path_name}: +{impact_kaggle:.3f} {'‚úì' if impact_kaggle > 0 else '‚ùå'}\")\n",
        "            else:\n",
        "                 print(f\"  Skipping '{path_name}': 'High' state not in query result for Efficiency.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not query for '{path_name}': {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 4 (Kaggle Data) VALIDATION COMPLETED!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeadd630"
      },
      "source": [
        "**Reasoning**:\n",
        "The error \"TypeError: unhashable type: 'list'\" occurs because the `get_cardinality` method is being called with a list `[var]` instead of the node name `var`. The `in` operator for checking if a state exists in the cardinality list is also incorrect. I need to fix how I access the cardinality and check for the existence of states.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwyW_QptvG3v"
      },
      "source": [
        "print(\"=== PHASE 4 (KAGGLE DATA): Model Inference & Validation ===\")\n",
        "print(\"=== Sub-phase 4.1: Sanity Checks ===\")\n",
        "\n",
        "# TEST 1: MARGINAL PROBABILITIES\n",
        "print(\"\\nüìä MARGINAL PROBABILITIES (Kaggle Data):\")\n",
        "# Check if the 'Efficiency' node exists in the model\n",
        "if 'Efficiency' in kaggle_model.nodes():\n",
        "    try:\n",
        "        efficiency_marginal_kaggle = kaggle_inference.query(variables=['Efficiency'])\n",
        "        print(\"Overall Efficiency Distribution:\")\n",
        "        for state, prob in zip(efficiency_marginal_kaggle.state_names['Efficiency'], efficiency_marginal_kaggle.values):\n",
        "            print(f\"  P({state}): {prob:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error querying marginal probability for Efficiency: {e}\")\n",
        "else:\n",
        "    print(\"  'Efficiency' node not found in the Kaggle model.\")\n",
        "\n",
        "\n",
        "# TEST 2: BASKETBALL LOGIC SCENARIOS (Kaggle Data)\n",
        "print(\"\\nüèÄ BASKETBALL LOGIC TESTS (Kaggle Data):\")\n",
        "\n",
        "# Elite shooting team\n",
        "evidence_elite_kaggle = {'Shooting_FG': 'High', 'Shooting_3PT': 'High'}\n",
        "valid_evidence_elite = True\n",
        "for var, state in evidence_elite_kaggle.items():\n",
        "    if var not in kaggle_model.nodes():\n",
        "        print(f\"  Cannot test Elite Shooting scenario: Variable '{var}' not in Kaggle model nodes.\")\n",
        "        valid_evidence_elite = False\n",
        "        break\n",
        "    # Correctly check if the state exists in the possible states for the variable\n",
        "    if state not in kaggle_model.get_state_names(var):\n",
        "        print(f\"  Cannot test Elite Shooting scenario: State '{state}' not present for '{var}' in Kaggle model.\")\n",
        "        valid_evidence_elite = False\n",
        "        break\n",
        "\n",
        "\n",
        "if valid_evidence_elite:\n",
        "    try:\n",
        "        result_elite_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence_elite_kaggle)\n",
        "        # Check if 'High' state exists in the query result for Efficiency\n",
        "        if 'High' in result_elite_kaggle.state_names['Efficiency']:\n",
        "            elite_high_kaggle = result_elite_kaggle.values[result_elite_kaggle.state_names['Efficiency'].index('High')]\n",
        "            print(f\"Elite Shooting ‚Üí P(High Efficiency): {elite_high_kaggle:.3f}\")\n",
        "        else:\n",
        "            print(f\"  'High' efficiency state not in result for Elite Shooting scenario.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not query for Elite Shooting: {e}\")\n",
        "\n",
        "# Poor ball control team\n",
        "evidence_poor_ball_kaggle = {'Playmaking': 'Low', 'Turnovers': 'High'}\n",
        "valid_evidence_poor_ball = True\n",
        "for var, state in evidence_poor_ball_kaggle.items():\n",
        "    if var not in kaggle_model.nodes():\n",
        "        print(f\"  Cannot test Poor Ball Control scenario: Variable '{var}' not in Kaggle model nodes.\")\n",
        "        valid_evidence_poor_ball = False\n",
        "        break\n",
        "    # Correctly check if the state exists in the possible states for the variable\n",
        "    if state not in kaggle_model.get_state_names(var):\n",
        "        print(f\"  Cannot test Poor Ball Control scenario: State '{state}' not present for '{var}' in Kaggle model.\")\n",
        "        valid_evidence_poor_ball = False\n",
        "        break\n",
        "\n",
        "if valid_evidence_poor_ball:\n",
        "    try:\n",
        "        result_poor_ball_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence_poor_ball_kaggle)\n",
        "        # Check if 'Low' state exists in the query result for Efficiency\n",
        "        if 'Low' in result_poor_ball_kaggle.state_names['Efficiency']:\n",
        "            poor_ball_low_kaggle = result_poor_ball_kaggle.values[result_poor_ball_kaggle.state_names['Efficiency'].index('Low')]\n",
        "            print(f\"Poor Ball Control ‚Üí P(Low Efficiency): {poor_ball_low_kaggle:.3f}\")\n",
        "        else:\n",
        "            print(f\"  'Low' efficiency state not in result for Poor Ball Control scenario.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not query for Poor Ball Control: {e}\")\n",
        "\n",
        "\n",
        "# TEST 3: NETWORK PATH VERIFICATION (Kaggle Data)\n",
        "print(\"\\nüîó NETWORK PATH VERIFICATION (Kaggle Data):\")\n",
        "paths_kaggle = [\n",
        "    (\"Shooting_FG ‚Üí Efficiency\", {'Shooting_FG': 'High'}),\n",
        "    (\"Shooting_3PT ‚Üí Efficiency\", {'Shooting_3PT': 'High'}),\n",
        "    (\"Playmaking ‚Üí Efficiency\", {'Playmaking': 'High'}),\n",
        "    (\"Turnovers ‚Üí Efficiency\", {'Turnovers': 'Low'}), # Low turnovers is good\n",
        "    (\"Offensive_Rebounding ‚Üí Efficiency\", {'Offensive_Rebounding': 'High'})\n",
        "]\n",
        "\n",
        "# Get baseline marginal probability for 'High' Efficiency if it exists\n",
        "baseline_kaggle_high = 0\n",
        "if 'Efficiency' in kaggle_model.nodes() and 'High' in kaggle_model.get_state_names('Efficiency'):\n",
        "     try:\n",
        "        baseline_marginal = kaggle_inference.query(variables=['Efficiency'])\n",
        "        if 'High' in baseline_marginal.state_names['Efficiency']:\n",
        "            baseline_kaggle_high = baseline_marginal.values[baseline_marginal.state_names['Efficiency'].index('High')]\n",
        "     except Exception as e:\n",
        "         print(f\"  Could not get baseline marginal for High Efficiency: {e}\")\n",
        "\n",
        "\n",
        "for path_name, evidence in paths_kaggle:\n",
        "    # Check if the evidence state exists in the model\n",
        "    valid_evidence = True\n",
        "    for var, state in evidence.items():\n",
        "        if var not in kaggle_model.nodes():\n",
        "             print(f\"  Skipping '{path_name}': Variable '{var}' not in Kaggle model nodes.\")\n",
        "             valid_evidence = False\n",
        "             break\n",
        "        if state not in kaggle_model.get_state_names(var):\n",
        "            print(f\"  Skipping '{path_name}': Evidence state '{state}' not present for '{var}' in Kaggle model.\")\n",
        "            valid_evidence = False\n",
        "            break\n",
        "\n",
        "    if valid_evidence:\n",
        "        try:\n",
        "            result_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence)\n",
        "            # Check if 'High' state exists in the query result\n",
        "            if 'High' in result_kaggle.state_names['Efficiency']:\n",
        "                high_prob_kaggle = result_kaggle.values[result_kaggle.state_names['Efficiency'].index('High')]\n",
        "                impact_kaggle = high_prob_kaggle - baseline_kaggle_high\n",
        "                print(f\"  {path_name}: +{impact_kaggle:.3f} {'‚úì' if impact_kaggle > 0 else '‚ùå'}\")\n",
        "            else:\n",
        "                 print(f\"  Skipping '{path_name}': 'High' state not in query result for Efficiency.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not query for '{path_name}': {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 4 (Kaggle Data) VALIDATION COMPLETED!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5924bdb4"
      },
      "source": [
        "**Reasoning**:\n",
        "The error \"AttributeError: 'DiscreteBayesianNetwork' object has no attribute 'get_state_names'\" indicates that the `get_state_names` method does not exist for the `DiscreteBayesianNetwork` object. I need to find the correct way to access the states of a variable in the pgmpy model. The states are stored within the CPDs of each node. I will modify the code to access the states through the CPDs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFovPKs6vNDa"
      },
      "source": [
        "print(\"=== PHASE 4 (KAGGLE DATA): Model Inference & Validation ===\")\n",
        "print(\"=== Sub-phase 4.1: Sanity Checks ===\")\n",
        "\n",
        "# TEST 1: MARGINAL PROBABILITIES\n",
        "print(\"\\nüìä MARGINAL PROBABILITIES (Kaggle Data):\")\n",
        "# Check if the 'Efficiency' node exists in the model\n",
        "if 'Efficiency' in kaggle_model.nodes():\n",
        "    try:\n",
        "        efficiency_marginal_kaggle = kaggle_inference.query(variables=['Efficiency'])\n",
        "        print(\"Overall Efficiency Distribution:\")\n",
        "        for state, prob in zip(efficiency_marginal_kaggle.state_names['Efficiency'], efficiency_marginal_kaggle.values):\n",
        "            print(f\"  P({state}): {prob:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error querying marginal probability for Efficiency: {e}\")\n",
        "else:\n",
        "    print(\"  'Efficiency' node not found in the Kaggle model.\")\n",
        "\n",
        "\n",
        "# TEST 2: BASKETBALL LOGIC SCENARIOS (Kaggle Data)\n",
        "print(\"\\nüèÄ BASKETBALL LOGIC TESTS (Kaggle Data):\")\n",
        "\n",
        "# Helper function to get states for a node\n",
        "def get_node_states(model, node):\n",
        "    \"\"\"Returns the list of states for a given node in the model.\"\"\"\n",
        "    try:\n",
        "        cpd = model.get_cpds(node)\n",
        "        return cpd.state_names[node]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "\n",
        "# Elite shooting team\n",
        "evidence_elite_kaggle = {'Shooting_FG': 'High', 'Shooting_3PT': 'High'}\n",
        "valid_evidence_elite = True\n",
        "for var, state in evidence_elite_kaggle.items():\n",
        "    if var not in kaggle_model.nodes():\n",
        "        print(f\"  Cannot test Elite Shooting scenario: Variable '{var}' not in Kaggle model nodes.\")\n",
        "        valid_evidence_elite = False\n",
        "        break\n",
        "    # Correctly check if the state exists in the possible states for the variable\n",
        "    if state not in get_node_states(kaggle_model, var):\n",
        "        print(f\"  Cannot test Elite Shooting scenario: State '{state}' not present for '{var}' in Kaggle model.\")\n",
        "        valid_evidence_elite = False\n",
        "        break\n",
        "\n",
        "\n",
        "if valid_evidence_elite:\n",
        "    try:\n",
        "        result_elite_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence_elite_kaggle)\n",
        "        # Check if 'High' state exists in the query result for Efficiency\n",
        "        if 'High' in result_elite_kaggle.state_names['Efficiency']:\n",
        "            elite_high_kaggle = result_elite_kaggle.values[result_elite_kaggle.state_names['Efficiency'].index('High')]\n",
        "            print(f\"Elite Shooting ‚Üí P(High Efficiency): {elite_high_kaggle:.3f}\")\n",
        "        else:\n",
        "            print(f\"  'High' efficiency state not in result for Elite Shooting scenario.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not query for Elite Shooting: {e}\")\n",
        "\n",
        "# Poor ball control team\n",
        "evidence_poor_ball_kaggle = {'Playmaking': 'Low', 'Turnovers': 'High'}\n",
        "valid_evidence_poor_ball = True\n",
        "for var, state in evidence_poor_ball_kaggle.items():\n",
        "    if var not in kaggle_model.nodes():\n",
        "        print(f\"  Cannot test Poor Ball Control scenario: Variable '{var}' not in Kaggle model nodes.\")\n",
        "        valid_evidence_poor_ball = False\n",
        "        break\n",
        "    # Correctly check if the state exists in the possible states for the variable\n",
        "    if state not in get_node_states(kaggle_model, var):\n",
        "        print(f\"  Cannot test Poor Ball Control scenario: State '{state}' not present for '{var}' in Kaggle model.\")\n",
        "        valid_evidence_poor_ball = False\n",
        "        break\n",
        "\n",
        "if valid_evidence_poor_ball:\n",
        "    try:\n",
        "        result_poor_ball_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence_poor_ball_kaggle)\n",
        "        # Check if 'Low' state exists in the query result for Efficiency\n",
        "        if 'Low' in result_poor_ball_kaggle.state_names['Efficiency']:\n",
        "            poor_ball_low_kaggle = result_poor_ball_kaggle.values[result_poor_ball_kaggle.state_names['Efficiency'].index('Low')]\n",
        "            print(f\"Poor Ball Control ‚Üí P(Low Efficiency): {poor_ball_low_kaggle:.3f}\")\n",
        "        else:\n",
        "            print(f\"  'Low' efficiency state not in result for Poor Ball Control scenario.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not query for Poor Ball Control: {e}\")\n",
        "\n",
        "\n",
        "# TEST 3: NETWORK PATH VERIFICATION (Kaggle Data)\n",
        "print(\"\\nüîó NETWORK PATH VERIFICATION (Kaggle Data):\")\n",
        "paths_kaggle = [\n",
        "    (\"Shooting_FG ‚Üí Efficiency\", {'Shooting_FG': 'High'}),\n",
        "    (\"Shooting_3PT ‚Üí Efficiency\", {'Shooting_3PT': 'High'}),\n",
        "    (\"Playmaking ‚Üí Efficiency\", {'Playmaking': 'High'}),\n",
        "    (\"Turnovers ‚Üí Efficiency\", {'Turnovers': 'Low'}), # Low turnovers is good\n",
        "    (\"Offensive_Rebounding ‚Üí Efficiency\", {'Offensive_Rebounding': 'High'})\n",
        "]\n",
        "\n",
        "# Get baseline marginal probability for 'High' Efficiency if it exists\n",
        "baseline_kaggle_high = 0\n",
        "if 'Efficiency' in kaggle_model.nodes() and 'High' in get_node_states(kaggle_model, 'Efficiency'):\n",
        "     try:\n",
        "        baseline_marginal = kaggle_inference.query(variables=['Efficiency'])\n",
        "        if 'High' in baseline_marginal.state_names['Efficiency']:\n",
        "            baseline_kaggle_high = baseline_marginal.values[baseline_marginal.state_names['Efficiency'].index('High')]\n",
        "     except Exception as e:\n",
        "         print(f\"  Could not get baseline marginal for High Efficiency: {e}\")\n",
        "\n",
        "\n",
        "for path_name, evidence in paths_kaggle:\n",
        "    # Check if the evidence state exists in the model\n",
        "    valid_evidence = True\n",
        "    for var, state in evidence.items():\n",
        "        if var not in kaggle_model.nodes():\n",
        "             print(f\"  Skipping '{path_name}': Variable '{var}' not in Kaggle model nodes.\")\n",
        "             valid_evidence = False\n",
        "             break\n",
        "        if state not in get_node_states(kaggle_model, var):\n",
        "            print(f\"  Skipping '{path_name}': Evidence state '{state}' not present for '{var}' in Kaggle model.\")\n",
        "            valid_evidence = False\n",
        "            break\n",
        "\n",
        "    if valid_evidence:\n",
        "        try:\n",
        "            result_kaggle = kaggle_inference.query(variables=['Efficiency'], evidence=evidence)\n",
        "            # Check if 'High' state exists in the query result\n",
        "            if 'High' in result_kaggle.state_names['Efficiency']:\n",
        "                high_prob_kaggle = result_kaggle.values[result_kaggle.state_names['Efficiency'].index('High')]\n",
        "                impact_kaggle = high_prob_kaggle - baseline_kaggle_high\n",
        "                print(f\"  {path_name}: +{impact_kaggle:.3f} {'‚úì' if impact_kaggle > 0 else '‚ùå'}\")\n",
        "            else:\n",
        "                 print(f\"  Skipping '{path_name}': 'High' state not in query result for Efficiency.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not query for '{path_name}': {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 4 (Kaggle Data) VALIDATION COMPLETED!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "322a54aa"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The Kaggle datasets (`games.csv`, `games_details.csv`, `players.csv`) were successfully loaded and merged to create a base DataFrame containing game and player details.\n",
        "*   A method to simulate lineup data by grouping players with the same minutes played within a game and team was implemented, focusing on 5-player groups. The turnover column name required correction from 'TOV' to 'TO' during aggregation.\n",
        "*   The initial cleaning and filtering of the Kaggle lineup data resulted in a DataFrame with 11 rows after filtering for lineups with more than 20 minutes played. No missing values were found in critical columns in this filtered subset.\n",
        "*   The initial \"smart\" discretization strategy applied to the Kaggle data resulted in the 'Efficiency' variable having only a single category ('Medium'), rendering subsequent Bayesian network learning and inference for 'Efficiency' states other than 'Medium' impossible.\n",
        "*   A revised, simpler discretization strategy using basic binning was implemented to address the small dataset size and limited unique values. This successfully created multiple categories (2 or 3) for all variables except 'Efficiency', which remained a single category ('Medium') due to the input data.\n",
        "*   The Bayesian Network structure was adapted to the available discretized variables from the Kaggle data, and CPTs were learned successfully using the Maximum Likelihood Estimator.\n",
        "*   Inference and validation tests were successfully adapted and run on the Kaggle-based Bayesian Network. However, the results confirmed that meaningful inference regarding 'High' or 'Low' efficiency was not possible due to the single state present in the 'Efficiency' node of the learned model.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current Kaggle dataset and the simple discretization strategy employed yield a \"flat\" Bayesian Network where 'Efficiency' is a constant. To build a functional predictive or analytical model for different levels of 'Efficiency', either a dataset with more variability in the relevant statistics (especially plus-minus) is needed, or a more sophisticated discretization method that can create multiple categories from limited data points (if statistically justifiable).\n",
        "*   Explore alternative methods for simulating lineups from the Kaggle data, as grouping solely by exact minutes played might not capture all on-court combinations and could limit the dataset size. Consider approaches based on substitution patterns or sequential analysis if feasible with the available data timestamps.\n"
      ]
    }
  ]
}